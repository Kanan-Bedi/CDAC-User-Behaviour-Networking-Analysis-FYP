{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6732710,"sourceType":"datasetVersion","datasetId":3833605},{"sourceId":9536528,"sourceType":"datasetVersion","datasetId":5808560},{"sourceId":9669706,"sourceType":"datasetVersion","datasetId":5908923},{"sourceId":9679946,"sourceType":"datasetVersion","datasetId":5916526}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T09:54:21.751997Z","iopub.execute_input":"2024-10-17T09:54:21.752580Z","iopub.status.idle":"2024-10-17T09:54:21.766172Z","shell.execute_reply.started":"2024-10-17T09:54:21.752549Z","shell.execute_reply":"2024-10-17T09:54:21.765176Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"/kaggle/input/cert-final/dataset/users.csv\n/kaggle/input/cert-final/dataset/logon.csv\n/kaggle/input/cert-final/dataset/decoyfile.csv\n/kaggle/input/cert-final/dataset/device.csv\n/kaggle/input/cert-final/dataset/psychometric.csv\n/kaggle/input/cert-final/dataset/http.csv\n/kaggle/input/cert-final/dataset/email.csv\n/kaggle/input/cert-final/dataset/file.csv\n/kaggle/input/cert-insider-threat-detection-research/users.csv\n/kaggle/input/cert-insider-threat-detection-research/logon.csv\n/kaggle/input/cert-insider-threat-detection-research/device.csv\n/kaggle/input/cert-insider-threat-detection-research/psychometric.csv\n/kaggle/input/cert-insider-threat-detection-research/http.csv\n/kaggle/input/cert-insider-threat-detection-research/email.csv\n/kaggle/input/cert-insider-threat-detection-research/file.csv\n/kaggle/input/cert-insider-threat-detection-research/decoy_file.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-08.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2011-03.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-10.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-02.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-03.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-09.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2011-04.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2009-12.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-06.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-07.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-05.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-11.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-04.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-12.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2011-01.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2011-02.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2011-05.csv\n/kaggle/input/cert-insider-threat-detection-research/LDAP/2010-01.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#https://docs.google.com/document/d/1KmA6PAfPHzkKyqErFboES37CBZKWv3lFRNDw4aFgXZs/edit?usp=sharing","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:21.768011Z","iopub.execute_input":"2024-10-17T09:54:21.768276Z","iopub.status.idle":"2024-10-17T09:54:21.772006Z","shell.execute_reply.started":"2024-10-17T09:54:21.768252Z","shell.execute_reply":"2024-10-17T09:54:21.771158Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:21.773117Z","iopub.execute_input":"2024-10-17T09:54:21.773455Z","iopub.status.idle":"2024-10-17T09:54:21.784862Z","shell.execute_reply.started":"2024-10-17T09:54:21.773429Z","shell.execute_reply":"2024-10-17T09:54:21.784005Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/decoy_file.csv',nrows=20000)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:21.786095Z","iopub.execute_input":"2024-10-17T09:54:21.786955Z","iopub.status.idle":"2024-10-17T09:54:21.826448Z","shell.execute_reply.started":"2024-10-17T09:54:21.786925Z","shell.execute_reply":"2024-10-17T09:54:21.825601Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:21.828797Z","iopub.execute_input":"2024-10-17T09:54:21.829102Z","iopub.status.idle":"2024-10-17T09:54:21.839966Z","shell.execute_reply.started":"2024-10-17T09:54:21.829078Z","shell.execute_reply":"2024-10-17T09:54:21.839042Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"                decoy_filename       pc\n10650  C:\\CJH2330\\6KG8PY0V.doc  PC-1768\n2041   C:\\JBP0651\\GEOQFL5E.doc  PC-0903\n8668           C:\\QUG5GCUL.doc  PC-0514\n1114   C:\\63DdkX0\\CK6FP9BC.doc  PC-8185\n13902  C:\\45NTT71\\EU0JQQDY.doc  PC-5968","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>decoy_filename</th>\n      <th>pc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10650</th>\n      <td>C:\\CJH2330\\6KG8PY0V.doc</td>\n      <td>PC-1768</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>C:\\JBP0651\\GEOQFL5E.doc</td>\n      <td>PC-0903</td>\n    </tr>\n    <tr>\n      <th>8668</th>\n      <td>C:\\QUG5GCUL.doc</td>\n      <td>PC-0514</td>\n    </tr>\n    <tr>\n      <th>1114</th>\n      <td>C:\\63DdkX0\\CK6FP9BC.doc</td>\n      <td>PC-8185</td>\n    </tr>\n    <tr>\n      <th>13902</th>\n      <td>C:\\45NTT71\\EU0JQQDY.doc</td>\n      <td>PC-5968</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/decoy_file_sample.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:21.840977Z","iopub.execute_input":"2024-10-17T09:54:21.841237Z","iopub.status.idle":"2024-10-17T09:54:21.850165Z","shell.execute_reply.started":"2024-10-17T09:54:21.841202Z","shell.execute_reply":"2024-10-17T09:54:21.849420Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/email.csv',nrows=20000)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:21.851155Z","iopub.execute_input":"2024-10-17T09:54:21.851443Z","iopub.status.idle":"2024-10-17T09:54:22.211236Z","shell.execute_reply.started":"2024-10-17T09:54:21.851419Z","shell.execute_reply":"2024-10-17T09:54:22.210432Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.212241Z","iopub.execute_input":"2024-10-17T09:54:22.212513Z","iopub.status.idle":"2024-10-17T09:54:22.229032Z","shell.execute_reply.started":"2024-10-17T09:54:22.212487Z","shell.execute_reply":"2024-10-17T09:54:22.228097Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                             id                 date     user       pc  \\\n10650  {A8Z4-G1RP17PX-9187CYDV}  01/04/2010 09:59:12  CRP1915  PC-9070   \n2041   {V5K6-M9MY26EO-0756IENH}  01/03/2010 10:01:57  BOR2387  PC-2983   \n8668   {P7F9-G5UX40IJ-1018XRLN}  01/04/2010 09:22:04  WVD0570  PC-7522   \n1114   {W1P3-F4KH73GO-4247PLUQ}  01/02/2010 13:36:59  JWM0484  PC-3083   \n13902  {X0L6-J8JS75DW-1673PFIC}  01/04/2010 11:03:11  HLZ2414  PC-3770   \n\n                                                    to   cc  bcc  \\\n10650                       Carolyn.Rina.Pena@dtaa.com  NaN  NaN   \n2041   Dana_Wilkins@msn.com;Fuentes-Elijah@comcast.net  NaN  NaN   \n8668                   Whitney.Vivien.Dejesus@dtaa.com  NaN  NaN   \n1114                 Jordan.Winifred.Mckenzie@dtaa.com  NaN  NaN   \n13902                  Hall.Leonard.Zimmerman@dtaa.com  NaN  NaN   \n\n                               from activity   size attachments  \\\n10650     Lucius.Noah.Kemp@dtaa.com     View  21887         NaN   \n2041              BOR14@verizon.net     Send  37102         NaN   \n8668               DEC83@harris.com     View  24132         NaN   \n1114   Aiko.Deborah.Mosley@dtaa.com     View  39482         NaN   \n13902   Jolene.Jordan.Lane@dtaa.com     View  33148         NaN   \n\n                                                 content  \n10650  It's often hard to tell if the game's backgrou...  \n2041   The score closes, at Whale's suggestion, with ...  \n8668   Born in 1902, the real-life Lad was an unregis...  \n1114   While he calls the album \"great traditional ro...  \n13902  from apes. {Note_label|C|III|none} See, for ex...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>user</th>\n      <th>pc</th>\n      <th>to</th>\n      <th>cc</th>\n      <th>bcc</th>\n      <th>from</th>\n      <th>activity</th>\n      <th>size</th>\n      <th>attachments</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10650</th>\n      <td>{A8Z4-G1RP17PX-9187CYDV}</td>\n      <td>01/04/2010 09:59:12</td>\n      <td>CRP1915</td>\n      <td>PC-9070</td>\n      <td>Carolyn.Rina.Pena@dtaa.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Lucius.Noah.Kemp@dtaa.com</td>\n      <td>View</td>\n      <td>21887</td>\n      <td>NaN</td>\n      <td>It's often hard to tell if the game's backgrou...</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>{V5K6-M9MY26EO-0756IENH}</td>\n      <td>01/03/2010 10:01:57</td>\n      <td>BOR2387</td>\n      <td>PC-2983</td>\n      <td>Dana_Wilkins@msn.com;Fuentes-Elijah@comcast.net</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>BOR14@verizon.net</td>\n      <td>Send</td>\n      <td>37102</td>\n      <td>NaN</td>\n      <td>The score closes, at Whale's suggestion, with ...</td>\n    </tr>\n    <tr>\n      <th>8668</th>\n      <td>{P7F9-G5UX40IJ-1018XRLN}</td>\n      <td>01/04/2010 09:22:04</td>\n      <td>WVD0570</td>\n      <td>PC-7522</td>\n      <td>Whitney.Vivien.Dejesus@dtaa.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>DEC83@harris.com</td>\n      <td>View</td>\n      <td>24132</td>\n      <td>NaN</td>\n      <td>Born in 1902, the real-life Lad was an unregis...</td>\n    </tr>\n    <tr>\n      <th>1114</th>\n      <td>{W1P3-F4KH73GO-4247PLUQ}</td>\n      <td>01/02/2010 13:36:59</td>\n      <td>JWM0484</td>\n      <td>PC-3083</td>\n      <td>Jordan.Winifred.Mckenzie@dtaa.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Aiko.Deborah.Mosley@dtaa.com</td>\n      <td>View</td>\n      <td>39482</td>\n      <td>NaN</td>\n      <td>While he calls the album \"great traditional ro...</td>\n    </tr>\n    <tr>\n      <th>13902</th>\n      <td>{X0L6-J8JS75DW-1673PFIC}</td>\n      <td>01/04/2010 11:03:11</td>\n      <td>HLZ2414</td>\n      <td>PC-3770</td>\n      <td>Hall.Leonard.Zimmerman@dtaa.com</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Jolene.Jordan.Lane@dtaa.com</td>\n      <td>View</td>\n      <td>33148</td>\n      <td>NaN</td>\n      <td>from apes. {Note_label|C|III|none} See, for ex...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/email_sample.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.230354Z","iopub.execute_input":"2024-10-17T09:54:22.231353Z","iopub.status.idle":"2024-10-17T09:54:22.255918Z","shell.execute_reply.started":"2024-10-17T09:54:22.231324Z","shell.execute_reply":"2024-10-17T09:54:22.255030Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/http.csv',nrows=20000)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.257202Z","iopub.execute_input":"2024-10-17T09:54:22.257535Z","iopub.status.idle":"2024-10-17T09:54:22.589432Z","shell.execute_reply.started":"2024-10-17T09:54:22.257504Z","shell.execute_reply":"2024-10-17T09:54:22.588638Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.590441Z","iopub.execute_input":"2024-10-17T09:54:22.590696Z","iopub.status.idle":"2024-10-17T09:54:22.605604Z","shell.execute_reply.started":"2024-10-17T09:54:22.590673Z","shell.execute_reply":"2024-10-17T09:54:22.604518Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"                             id                 date     user       pc  \\\n10650  {C8L3-V7BT35GJ-3741ASFK}  01/02/2010 14:02:33  LCC1963  PC-1027   \n2041   {R5E1-B1TL97WP-3937CAUC}  01/02/2010 08:44:28  ATT1191  PC-2057   \n8668   {D4T5-Z0TX04MG-3718PMQQ}  01/02/2010 12:49:12  EDJ1970  PC-1301   \n1114   {L7L2-K8TH89CO-4400MEMC}  01/02/2010 08:15:59  SFC2336  PC-1921   \n13902  {V6H1-G9KX32SM-7653CHFK}  01/02/2010 15:48:23  NDK3436  PC-9671   \n\n                                                     url   activity  \\\n10650  http://hubspot.com/Utah_State_Route_128/kokope...  WWW Visit   \n2041   http://domaintools.com/Ross_Perot_presidential...  WWW Visit   \n8668   http://guardian.co.uk/Thoroughbred/thoroughbre...  WWW Visit   \n1114   http://perezhilton.com/Procellariidae/fulmarin...  WWW Visit   \n13902  http://thedailybeast.com/Ambondro_mahabo/austr...  WWW Visit   \n\n                                                 content  \n10650  The South African-born English cricketer Jonat...  \n2041   A dash generated a positive charge, and a dot ...  \n8668   Wolf recovers and is given a \"Hero Cup\" trophy...  \n1114   Carey Sublette of Nuclear Weapon Archive argue...  \n13902  The road was constructed by the county road co...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>user</th>\n      <th>pc</th>\n      <th>url</th>\n      <th>activity</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10650</th>\n      <td>{C8L3-V7BT35GJ-3741ASFK}</td>\n      <td>01/02/2010 14:02:33</td>\n      <td>LCC1963</td>\n      <td>PC-1027</td>\n      <td>http://hubspot.com/Utah_State_Route_128/kokope...</td>\n      <td>WWW Visit</td>\n      <td>The South African-born English cricketer Jonat...</td>\n    </tr>\n    <tr>\n      <th>2041</th>\n      <td>{R5E1-B1TL97WP-3937CAUC}</td>\n      <td>01/02/2010 08:44:28</td>\n      <td>ATT1191</td>\n      <td>PC-2057</td>\n      <td>http://domaintools.com/Ross_Perot_presidential...</td>\n      <td>WWW Visit</td>\n      <td>A dash generated a positive charge, and a dot ...</td>\n    </tr>\n    <tr>\n      <th>8668</th>\n      <td>{D4T5-Z0TX04MG-3718PMQQ}</td>\n      <td>01/02/2010 12:49:12</td>\n      <td>EDJ1970</td>\n      <td>PC-1301</td>\n      <td>http://guardian.co.uk/Thoroughbred/thoroughbre...</td>\n      <td>WWW Visit</td>\n      <td>Wolf recovers and is given a \"Hero Cup\" trophy...</td>\n    </tr>\n    <tr>\n      <th>1114</th>\n      <td>{L7L2-K8TH89CO-4400MEMC}</td>\n      <td>01/02/2010 08:15:59</td>\n      <td>SFC2336</td>\n      <td>PC-1921</td>\n      <td>http://perezhilton.com/Procellariidae/fulmarin...</td>\n      <td>WWW Visit</td>\n      <td>Carey Sublette of Nuclear Weapon Archive argue...</td>\n    </tr>\n    <tr>\n      <th>13902</th>\n      <td>{V6H1-G9KX32SM-7653CHFK}</td>\n      <td>01/02/2010 15:48:23</td>\n      <td>NDK3436</td>\n      <td>PC-9671</td>\n      <td>http://thedailybeast.com/Ambondro_mahabo/austr...</td>\n      <td>WWW Visit</td>\n      <td>The road was constructed by the county road co...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/http.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.606908Z","iopub.execute_input":"2024-10-17T09:54:22.607710Z","iopub.status.idle":"2024-10-17T09:54:22.633191Z","shell.execute_reply.started":"2024-10-17T09:54:22.607674Z","shell.execute_reply":"2024-10-17T09:54:22.632253Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/psychometric.csv',nrows=20000)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.634629Z","iopub.execute_input":"2024-10-17T09:54:22.635310Z","iopub.status.idle":"2024-10-17T09:54:22.658279Z","shell.execute_reply.started":"2024-10-17T09:54:22.635270Z","shell.execute_reply":"2024-10-17T09:54:22.657507Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.662744Z","iopub.execute_input":"2024-10-17T09:54:22.663066Z","iopub.status.idle":"2024-10-17T09:54:22.675583Z","shell.execute_reply.started":"2024-10-17T09:54:22.663039Z","shell.execute_reply":"2024-10-17T09:54:22.674621Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"                 employee_name  user_id   O   C   E   A   N\n555            Susan Clare Day  SCD3192  12  46  35  40  28\n3491          Gail Orla Graves  GOG3939  38  17  48  42  27\n527        Tashya Taylor Reese  TTR0045  14  19  28  23  38\n3925  Caldwell Baxter Calderon  CBC3175  45  45  44  22  35\n2989        Geoffrey Hu Morgan  GHM3953  34  38  18  12  33","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>employee_name</th>\n      <th>user_id</th>\n      <th>O</th>\n      <th>C</th>\n      <th>E</th>\n      <th>A</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>555</th>\n      <td>Susan Clare Day</td>\n      <td>SCD3192</td>\n      <td>12</td>\n      <td>46</td>\n      <td>35</td>\n      <td>40</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3491</th>\n      <td>Gail Orla Graves</td>\n      <td>GOG3939</td>\n      <td>38</td>\n      <td>17</td>\n      <td>48</td>\n      <td>42</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>Tashya Taylor Reese</td>\n      <td>TTR0045</td>\n      <td>14</td>\n      <td>19</td>\n      <td>28</td>\n      <td>23</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>3925</th>\n      <td>Caldwell Baxter Calderon</td>\n      <td>CBC3175</td>\n      <td>45</td>\n      <td>45</td>\n      <td>44</td>\n      <td>22</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>2989</th>\n      <td>Geoffrey Hu Morgan</td>\n      <td>GHM3953</td>\n      <td>34</td>\n      <td>38</td>\n      <td>18</td>\n      <td>12</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/psychometric.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.676729Z","iopub.execute_input":"2024-10-17T09:54:22.677028Z","iopub.status.idle":"2024-10-17T09:54:22.686885Z","shell.execute_reply.started":"2024-10-17T09:54:22.677001Z","shell.execute_reply":"2024-10-17T09:54:22.685958Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/decoy_file.csv',nrows=20000)\n# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()\n# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/decoy_file.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.687872Z","iopub.execute_input":"2024-10-17T09:54:22.688141Z","iopub.status.idle":"2024-10-17T09:54:22.716107Z","shell.execute_reply.started":"2024-10-17T09:54:22.688116Z","shell.execute_reply":"2024-10-17T09:54:22.715181Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/device.csv',nrows=20000)\n# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()\n# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/device.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.717238Z","iopub.execute_input":"2024-10-17T09:54:22.717498Z","iopub.status.idle":"2024-10-17T09:54:22.790041Z","shell.execute_reply.started":"2024-10-17T09:54:22.717473Z","shell.execute_reply":"2024-10-17T09:54:22.789056Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/file.csv',nrows=20000)\n# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()\n# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/file.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:22.791256Z","iopub.execute_input":"2024-10-17T09:54:22.791514Z","iopub.status.idle":"2024-10-17T09:54:23.093805Z","shell.execute_reply.started":"2024-10-17T09:54:22.791491Z","shell.execute_reply":"2024-10-17T09:54:23.093053Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Load the CSV data into a DataFrame\ndf = pd.read_csv('/kaggle/input/cert-insider-threat-detection-research/logon.csv',nrows=20000)\n# Randomly sample 500 rows\ndf_sampled = df.sample(n=500, random_state=42)  # Setting random_state ensures reproducibility\n\n# Display the sampled DataFrame\ndf_sampled.head()\n# Save the sampled data to a CSV file in the Kaggle directory\ndf_sampled.to_csv('/kaggle/working/logon.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.094803Z","iopub.execute_input":"2024-10-17T09:54:23.095116Z","iopub.status.idle":"2024-10-17T09:54:23.166615Z","shell.execute_reply.started":"2024-10-17T09:54:23.095089Z","shell.execute_reply":"2024-10-17T09:54:23.165832Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Folder containing the files\nfolder_path = '/kaggle/input/cert-final/dataset'\n\n# Initialize an empty list to store dataframes\ndf_list = []\n\n# Iterate through all files in the folder\nfor filename in os.listdir(folder_path):\n    if filename.endswith('.csv') or filename.endswith('.xls') or filename.endswith('.xlsx'):\n        file_path = os.path.join(folder_path, filename)\n\n        # Read the file based on its extension\n        if filename.endswith('.csv'):\n            df = pd.read_csv(file_path)\n        elif filename.endswith('.xls') or filename.endswith('.xlsx'):\n            df = pd.read_excel(file_path)\n\n        # Append dataframe to the list\n        df_list.append(df)\n\n# Concatenate all dataframes (outer join to include all columns)\nmerged_df = pd.concat(df_list, axis=0, ignore_index=True)\n\n# Drop rows where all elements are NaN (optional: drop based on specific columns)\ncleaned_df = merged_df.dropna(how='all')\n\n# Output the head of the cleaned dataframe\nprint(cleaned_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.167701Z","iopub.execute_input":"2024-10-17T09:54:23.168016Z","iopub.status.idle":"2024-10-17T09:54:23.277217Z","shell.execute_reply.started":"2024-10-17T09:54:23.167982Z","shell.execute_reply":"2024-10-17T09:54:23.276310Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"              employee_name  user_id                              email  \\\n0           Susan Clare Day  SCD3192           Susan.Clare.Day@dtaa.com   \n1          Gail Orla Graves  GOG3939          Gail.Orla.Graves@dtaa.com   \n2       Tashya Taylor Reese  TTR0045       Tashya.Taylor.Reese@dtaa.com   \n3  Caldwell Baxter Calderon  CBC3175  Caldwell.Baxter.Calderon@dtaa.com   \n4        Geoffrey Hu Morgan  GHM3953        Geoffrey.Hu.Morgan@dtaa.com   \n\n                   role projects  business_unit  \\\n0         Mathematician      NaN            1.0   \n1  ProductionLineWorker      NaN            1.0   \n2               ITAdmin      NaN            1.0   \n3      HardwareEngineer      NaN            1.0   \n4  ProductionLineWorker      NaN            1.0   \n\n                                  functional_unit       department  \\\n0   4 - ResearchAndEngineering_Government_Foreign     2 - Research   \n1                    5 - Manufacturing_Commercial     3 - Assembly   \n2                               1 - Adminstration     5 - Security   \n3  3 - ResearchAndEngineering_Government_Domestic  4 - Engineering   \n4                    5 - Manufacturing_Commercial     3 - Assembly   \n\n                      team              supervisor  ... content   to   cc  \\\n0                  5 - Lab  Geoffrey Rahim Griffin  ...     NaN  NaN  NaN   \n1        14 - AssemblyDept       Keegan Noah Riggs  ...     NaN  NaN  NaN   \n2   2 - ElectronicSecurity     Carl Garrett Morgan  ...     NaN  NaN  NaN   \n3  24 - SystemsEngineering    Malcolm Elton Battle  ...     NaN  NaN  NaN   \n4        14 - AssemblyDept       Keegan Noah Riggs  ...     NaN  NaN  NaN   \n\n   bcc from size attachments filename to_removable_media  from_removable_media  \n0  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n1  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n2  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n3  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n4  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n\n[5 rows x 35 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Folder containing the files\nfolder_path = '/kaggle/input/cert-final/dataset'\n\n# Initialize an empty list to store dataframes\ndf_list = []\n\n# Iterate through all files in the folder\nfor filename in os.listdir(folder_path):\n    if filename.endswith('.csv') or filename.endswith('.xls') or filename.endswith('.xlsx'):\n        file_path = os.path.join(folder_path, filename)\n\n        # Read the file based on its extension\n        if filename.endswith('.csv'):\n            df = pd.read_csv(file_path)\n        elif filename.endswith('.xls') or filename.endswith('.xlsx'):\n            df = pd.read_excel(file_path)\n\n        # Append dataframe to the list\n        df_list.append(df)\n\n# Concatenate all dataframes\nmerged_df = pd.concat(df_list, axis=0, ignore_index=True)\n\n# Drop rows with any NaN values\ncleaned_df = merged_df.dropna(how='all')\n\n# Output the head of the cleaned dataframe\nprint(cleaned_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.278562Z","iopub.execute_input":"2024-10-17T09:54:23.278943Z","iopub.status.idle":"2024-10-17T09:54:23.346778Z","shell.execute_reply.started":"2024-10-17T09:54:23.278907Z","shell.execute_reply":"2024-10-17T09:54:23.345871Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"              employee_name  user_id                              email  \\\n0           Susan Clare Day  SCD3192           Susan.Clare.Day@dtaa.com   \n1          Gail Orla Graves  GOG3939          Gail.Orla.Graves@dtaa.com   \n2       Tashya Taylor Reese  TTR0045       Tashya.Taylor.Reese@dtaa.com   \n3  Caldwell Baxter Calderon  CBC3175  Caldwell.Baxter.Calderon@dtaa.com   \n4        Geoffrey Hu Morgan  GHM3953        Geoffrey.Hu.Morgan@dtaa.com   \n\n                   role projects  business_unit  \\\n0         Mathematician      NaN            1.0   \n1  ProductionLineWorker      NaN            1.0   \n2               ITAdmin      NaN            1.0   \n3      HardwareEngineer      NaN            1.0   \n4  ProductionLineWorker      NaN            1.0   \n\n                                  functional_unit       department  \\\n0   4 - ResearchAndEngineering_Government_Foreign     2 - Research   \n1                    5 - Manufacturing_Commercial     3 - Assembly   \n2                               1 - Adminstration     5 - Security   \n3  3 - ResearchAndEngineering_Government_Domestic  4 - Engineering   \n4                    5 - Manufacturing_Commercial     3 - Assembly   \n\n                      team              supervisor  ... content   to   cc  \\\n0                  5 - Lab  Geoffrey Rahim Griffin  ...     NaN  NaN  NaN   \n1        14 - AssemblyDept       Keegan Noah Riggs  ...     NaN  NaN  NaN   \n2   2 - ElectronicSecurity     Carl Garrett Morgan  ...     NaN  NaN  NaN   \n3  24 - SystemsEngineering    Malcolm Elton Battle  ...     NaN  NaN  NaN   \n4        14 - AssemblyDept       Keegan Noah Riggs  ...     NaN  NaN  NaN   \n\n   bcc from size attachments filename to_removable_media  from_removable_media  \n0  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n1  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n2  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n3  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n4  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n\n[5 rows x 35 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Folder containing the files\nfolder_path = '/kaggle/input/cert-final/dataset'\n\n# Initialize an empty list to store dataframes\ndf_list = []\n\n# Iterate through all files in the folder\nfor filename in os.listdir(folder_path):\n    if filename.endswith('.csv') or filename.endswith('.xls') or filename.endswith('.xlsx'):\n        file_path = os.path.join(folder_path, filename)\n\n        # Read the file based on its extension\n        if filename.endswith('.csv'):\n            df = pd.read_csv(file_path)\n        elif filename.endswith('.xls') or filename.endswith('.xlsx'):\n            df = pd.read_excel(file_path)\n\n        # Append dataframe to the list\n        df_list.append(df)\n\n# Merge all dataframes in the list into one dataframe\n# Perform outer join to combine on all rows and columns\nmerged_df = pd.concat(df_list, axis=0, ignore_index=True)\n\n# Option 1: Drop rows where **all** values are NaN\ncleaned_df = merged_df.dropna(how='all')\n\n# Option 2: Drop rows where important columns have NaN values (e.g., 'employee_name', 'email')\ncleaned_df = cleaned_df.dropna(subset=['employee_name', 'email'], how='any')\n\n# Option 3: If you want to **fill** NaN values instead of dropping them\n# cleaned_df = cleaned_df.fillna('')  # Fills NaN with empty string\n\n# Display the first few rows to verify the result\nprint(cleaned_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.348129Z","iopub.execute_input":"2024-10-17T09:54:23.348481Z","iopub.status.idle":"2024-10-17T09:54:23.420064Z","shell.execute_reply.started":"2024-10-17T09:54:23.348445Z","shell.execute_reply":"2024-10-17T09:54:23.419217Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"              employee_name  user_id                              email  \\\n0           Susan Clare Day  SCD3192           Susan.Clare.Day@dtaa.com   \n1          Gail Orla Graves  GOG3939          Gail.Orla.Graves@dtaa.com   \n2       Tashya Taylor Reese  TTR0045       Tashya.Taylor.Reese@dtaa.com   \n3  Caldwell Baxter Calderon  CBC3175  Caldwell.Baxter.Calderon@dtaa.com   \n4        Geoffrey Hu Morgan  GHM3953        Geoffrey.Hu.Morgan@dtaa.com   \n\n                   role projects  business_unit  \\\n0         Mathematician      NaN            1.0   \n1  ProductionLineWorker      NaN            1.0   \n2               ITAdmin      NaN            1.0   \n3      HardwareEngineer      NaN            1.0   \n4  ProductionLineWorker      NaN            1.0   \n\n                                  functional_unit       department  \\\n0   4 - ResearchAndEngineering_Government_Foreign     2 - Research   \n1                    5 - Manufacturing_Commercial     3 - Assembly   \n2                               1 - Adminstration     5 - Security   \n3  3 - ResearchAndEngineering_Government_Domestic  4 - Engineering   \n4                    5 - Manufacturing_Commercial     3 - Assembly   \n\n                      team              supervisor  ... content   to   cc  \\\n0                  5 - Lab  Geoffrey Rahim Griffin  ...     NaN  NaN  NaN   \n1        14 - AssemblyDept       Keegan Noah Riggs  ...     NaN  NaN  NaN   \n2   2 - ElectronicSecurity     Carl Garrett Morgan  ...     NaN  NaN  NaN   \n3  24 - SystemsEngineering    Malcolm Elton Battle  ...     NaN  NaN  NaN   \n4        14 - AssemblyDept       Keegan Noah Riggs  ...     NaN  NaN  NaN   \n\n   bcc from size attachments filename to_removable_media  from_removable_media  \n0  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n1  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n2  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n3  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n4  NaN  NaN  NaN         NaN      NaN                NaN                   NaN  \n\n[5 rows x 35 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nfrom pathlib import Path\nimport re\nfrom gensim.models import TfidfModel, nmf\nfrom gensim.corpora import Dictionary as Dict\nfrom gensim.models.ldamulticore import LdaModel\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom gensim.models.nmf_pgd import solve_h","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.421253Z","iopub.execute_input":"2024-10-17T09:54:23.421619Z","iopub.status.idle":"2024-10-17T09:54:23.427727Z","shell.execute_reply.started":"2024-10-17T09:54:23.421583Z","shell.execute_reply":"2024-10-17T09:54:23.426728Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"CHUNK_SIZE = 10000\nTOPIC_NUM = 100\nALLFILES = ['logon.csv','device.csv', 'email.csv', 'file.csv', 'http.csv']\nCONTENT_FILES = ['email.csv', 'file.csv', 'http.csv']\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.428808Z","iopub.execute_input":"2024-10-17T09:54:23.429160Z","iopub.status.idle":"2024-10-17T09:54:23.437368Z","shell.execute_reply.started":"2024-10-17T09:54:23.429125Z","shell.execute_reply":"2024-10-17T09:54:23.436529Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def check():\n    if not output_dir.is_dir():\n        os.makedirs(output_dir)\n    assert (answers_dir.is_dir())\n    assert (dataset_dir.is_dir())\n    assert (main_answers_file.is_file())\n    assert (output_dir.is_dir())","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.438674Z","iopub.execute_input":"2024-10-17T09:54:23.439060Z","iopub.status.idle":"2024-10-17T09:54:23.446794Z","shell.execute_reply.started":"2024-10-17T09:54:23.439018Z","shell.execute_reply":"2024-10-17T09:54:23.445896Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"def count_file_lines(file):\n    with open(file) as f:\n        for count, _ in enumerate(f):\n            pass\n    return count","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.447980Z","iopub.execute_input":"2024-10-17T09:54:23.448249Z","iopub.status.idle":"2024-10-17T09:54:23.460640Z","shell.execute_reply.started":"2024-10-17T09:54:23.448224Z","shell.execute_reply":"2024-10-17T09:54:23.459740Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def collect_vocabulary(csv_name):\n    result_set = set()\n\n    for df in pd.read_csv(dataset_dir / f'{csv_name}.csv',\n                          usecols=['date', 'user', 'content'], chunksize=CHUNK_SIZE):\n        df['content'] = df['content'].str.lower().str.split()\n        result_set = result_set.union(*map(set, df['content']))\n\n    return result_set","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.461915Z","iopub.execute_input":"2024-10-17T09:54:23.462278Z","iopub.status.idle":"2024-10-17T09:54:23.471114Z","shell.execute_reply.started":"2024-10-17T09:54:23.462252Z","shell.execute_reply":"2024-10-17T09:54:23.470157Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def chunk_iterator(filename):\n    for chunk in pd.read_csv(filename, chunksize=CHUNK_SIZE):\n        for document in chunk['content'].str.lower().str.split().values:\n            yield document","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.472285Z","iopub.execute_input":"2024-10-17T09:54:23.472552Z","iopub.status.idle":"2024-10-17T09:54:23.482215Z","shell.execute_reply.started":"2024-10-17T09:54:23.472528Z","shell.execute_reply":"2024-10-17T09:54:23.481293Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def tfidf_iterator(filenames, dictionary):\n    for filename in filenames:\n        for chunk in pd.read_csv(dataset_dir /filename, chunksize=CHUNK_SIZE):\n            for document in chunk['content'].str.lower().str.split().values:\n                yield dictionary.doc2bow(document)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.483541Z","iopub.execute_input":"2024-10-17T09:54:23.484078Z","iopub.status.idle":"2024-10-17T09:54:23.491875Z","shell.execute_reply.started":"2024-10-17T09:54:23.484044Z","shell.execute_reply":"2024-10-17T09:54:23.491015Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"def nmf_iterator(filenames, dictionary, tfidf):\n    for filename in filenames:\n        for chunk in pd.read_csv(dataset_dir /filename, chunksize=CHUNK_SIZE):\n            for document in chunk['content'].str.lower().str.split().values:\n                yield tfidf[dictionary.doc2bow(document)]","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.493190Z","iopub.execute_input":"2024-10-17T09:54:23.493800Z","iopub.status.idle":"2024-10-17T09:54:23.502334Z","shell.execute_reply.started":"2024-10-17T09:54:23.493766Z","shell.execute_reply":"2024-10-17T09:54:23.501450Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Parallelism\n\ndef parallelize(data, func, num_of_processes=8):\n    data_split = np.array_split(data, num_of_processes)\n    pool = Pool(num_of_processes)\n    data = pd.concat(pool.map(func, data_split))\n    pool.close()\n    pool.join()\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.503521Z","iopub.execute_input":"2024-10-17T09:54:23.503824Z","iopub.status.idle":"2024-10-17T09:54:23.513054Z","shell.execute_reply.started":"2024-10-17T09:54:23.503798Z","shell.execute_reply":"2024-10-17T09:54:23.512024Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"def run_on_subset(func, data_subset):\n    return data_subset.apply(func, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.514311Z","iopub.execute_input":"2024-10-17T09:54:23.514570Z","iopub.status.idle":"2024-10-17T09:54:23.522633Z","shell.execute_reply.started":"2024-10-17T09:54:23.514546Z","shell.execute_reply":"2024-10-17T09:54:23.521917Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def parallelize_on_rows(data, func, num_of_processes=8):\n    return parallelize(data, partial(run_on_subset, func), num_of_processes)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.524089Z","iopub.execute_input":"2024-10-17T09:54:23.524418Z","iopub.status.idle":"2024-10-17T09:54:23.532948Z","shell.execute_reply.started":"2024-10-17T09:54:23.524384Z","shell.execute_reply":"2024-10-17T09:54:23.532072Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def process_content(filename, chunk_size=CHUNK_SIZE):\n    model = LdaModel.load((output_dir / 'lda_model.pkl').as_posix())\n    temp_dict = Dict.load((output_dir / 'dict.pkl').as_posix())\n    out_file = output_dir / (filename.stem + '_lda.csv')\n    # touch one while not exist\n    if not out_file.is_file():\n        Path(out_file).touch()\n\n    for chunk in pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size):\n        chunk['content'] = chunk['content'].str.lower().str.split() \\\n            .apply(lambda doc: model[temp_dict.doc2bow(doc)])\n\n        chunk.to_csv(out_file, mode='a')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.534203Z","iopub.execute_input":"2024-10-17T09:54:23.534893Z","iopub.status.idle":"2024-10-17T09:54:23.544552Z","shell.execute_reply.started":"2024-10-17T09:54:23.534842Z","shell.execute_reply":"2024-10-17T09:54:23.543614Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"\ndef pre_process_logon(path):\n    df = pd.read_csv(path)\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S').dt.floor('D')\n    df['day']=df['date'].dt.dayofweek\n    \n    self_pc = df \\\n        .groupby(['user', 'day', 'pc']).size().to_frame('count') \\\n        .reset_index().sort_values('count', ascending=False) \\\n        .drop_duplicates(subset=['user', 'day']) \\\n        .drop(columns=['count']).sort_values(['user', 'day']) \\\n        .groupby('user').pc.agg(pd.Series.mode).rename('self_pc')\n    df = df.merge(self_pc.to_frame(), left_on='user', right_on='user')\n    print(\"Done\")\n    print(df.head()) \n    #df['is_usual_pc'] = df['self_pc'] == df['pc']\n   \n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n    \n    df['subtype'] = df['activity']\n    #df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / 'logon_preprocessed.csv')\n    df[['id', 'date', 'user', 'is_work_time', 'subtype']].to_csv(output_dir / 'logon_preprocessed.csv')\n    return self_pc.to_frame()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.545456Z","iopub.execute_input":"2024-10-17T09:54:23.545699Z","iopub.status.idle":"2024-10-17T09:54:23.558807Z","shell.execute_reply.started":"2024-10-17T09:54:23.545676Z","shell.execute_reply":"2024-10-17T09:54:23.557899Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def pre_process_device(path):\n    df = pd.read_csv(path)\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n    df = df.merge(self_pc, left_on='user', right_on='user', )\n    #df['is_usual_pc'] = df['self_pc'] == df['pc']\n\n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n\n    df['subtype'] = df['activity']\n    #df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n    #    output_dir / f'device_preprocessed.csv')\n    df[['id', 'date', 'user', 'is_work_time', 'subtype']].to_csv(\n        output_dir / f'device_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.560078Z","iopub.execute_input":"2024-10-17T09:54:23.560624Z","iopub.status.idle":"2024-10-17T09:54:23.569720Z","shell.execute_reply.started":"2024-10-17T09:54:23.560596Z","shell.execute_reply":"2024-10-17T09:54:23.568880Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def pre_process_file(path):\n    df = pd.read_csv(path, usecols=['id', 'date', 'user', 'pc', 'filename'])\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n\n    df = df.merge(self_pc, left_on='user', right_on='user', )\n    #df['is_usual_pc'] = df['self_pc'] == df['pc']\n\n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n\n    file_extensions = df.filename.str[-4:]\n    df['subtype'] = file_extensions\n    #df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n    #    output_dir / f'file_preprocessed.csv')\n    df[['id', 'date', 'user', 'is_work_time', 'subtype']].to_csv(\n        output_dir / f'file_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.570890Z","iopub.execute_input":"2024-10-17T09:54:23.571210Z","iopub.status.idle":"2024-10-17T09:54:23.582369Z","shell.execute_reply.started":"2024-10-17T09:54:23.571178Z","shell.execute_reply":"2024-10-17T09:54:23.581568Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"def pre_process_email(path):\n    df = pd.read_csv(path, usecols=['id', 'date', 'user', 'pc', 'to', 'cc', 'bcc', 'from'])\n    df = df.fillna('')\n    to_concated = df[['to', 'cc', 'bcc']].apply(lambda x: ';'.join([x.to, x.cc, x.bcc]), axis=1)\n    is_external_to = to_concated.apply(\n        lambda x: any([re.match('^.+@(.+$)', e).group(1) != 'dtaa.com' for e in x.split(';') if e != '']))\n    is_external = is_external_to | is_external_to\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n\n    df = df.merge(self_pc, left_on='user', right_on='user', )\n    df['is_usual_pc'] = df['self_pc'] == df['pc']\n\n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n\n    df['subtype'] = is_external\n    df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n        output_dir / f'email_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.583329Z","iopub.execute_input":"2024-10-17T09:54:23.583638Z","iopub.status.idle":"2024-10-17T09:54:23.592734Z","shell.execute_reply.started":"2024-10-17T09:54:23.583613Z","shell.execute_reply":"2024-10-17T09:54:23.591915Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"def pre_process_http(path):\n\n    # scenario 1\n    scenario_1_http = [\n        'actualkeylogger.com',\n        'best-spy-soft.com',\n        'dailykeylogger.com',\n        'keylogpc.com',\n        'refog.com',\n        'relytec.com',\n        'softactivity.com',\n        'spectorsoft.com',\n        'webwatchernow.com',\n        'wellresearchedreviews.com',\n        'wikileaks.org'\n    ]\n\n    # scenario 2\n    scenario_2_http = [\n        'careerbuilder.com',\n        'craiglist.org',\n        'indeed.com',\n        'job-hunt.org',\n        'jobhuntersbible.com',\n        'linkedin.com',\n        'monster.com',\n        'simplyhired.com',\n    ]\n\n\n    #scenario 3\n    scenario_3_http = [\n        '4shared.com'\n        'dropbox.com',\n        'fileserve.com',\n        'filefreak.com',\n        'filestube.com',\n        'megaupload.com',\n        'thepiratebay.org'\n    ]\n\n    first_it = True\n    mode = 'w'\n\n    for http_df in pd.read_csv(path, chunksize=CHUNK_SIZE, usecols=['id','date', 'user', 'pc', 'url']):\n        http_df['date'] = pd.to_datetime(http_df.date, format='%m/%d/%Y %H:%M:%S')\n\n        site_names = http_df['url'].apply(lambda s: re.match('^https?://(www)?([0-9\\-\\w\\.]+)?.+$', s).group(2))\n        http_df['site_name'] = site_names\n\n        http_df['subtype'] = 0\n        http_df.loc[site_names.isin(scenario_1_http), 'subtype'] = 1\n        http_df.loc[site_names.isin(scenario_2_http), 'subtype'] = 2\n        http_df.loc[site_names.isin(scenario_3_http), 'subtype'] = 3\n\n        http_df = http_df.merge(self_pc, left_on='user', right_on='user', )\n        #http_df['is_usual_pc'] = http_df['self_pc'] == http_df['pc']\n\n        is_work_time = (8 <= http_df.date.dt.hour) & (http_df.date.dt.hour < 17)\n        http_df['is_work_time'] = is_work_time\n\n        http_df.to_csv(output_dir / 'http_preprocessed.csv', header=first_it, index=False,\n                       mode=mode, columns=['id', 'date', 'user', 'is_work_time', 'subtype', 'site_name'])\n        first_it = False\n        mode = 'a'\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.593863Z","iopub.execute_input":"2024-10-17T09:54:23.594177Z","iopub.status.idle":"2024-10-17T09:54:23.607477Z","shell.execute_reply.started":"2024-10-17T09:54:23.594128Z","shell.execute_reply":"2024-10-17T09:54:23.606685Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"def merge_all_content():\n    df_dict = Dict(chunk_iterator(dataset_dir / 'email.csv'))\n    df_dict.add_documents(chunk_iterator(dataset_dir / 'file.csv'))\n    df_dict.add_documents(chunk_iterator(dataset_dir / 'http.csv'))\n\n    df_dict.save((output_dir / 'dict.pkl').as_posix())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.608618Z","iopub.execute_input":"2024-10-17T09:54:23.608946Z","iopub.status.idle":"2024-10-17T09:54:23.621773Z","shell.execute_reply.started":"2024-10-17T09:54:23.608919Z","shell.execute_reply":"2024-10-17T09:54:23.620878Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def make_tfidf_model():\n    tfidf_model = TfidfModel(\n        tfidf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix())))\n\n    tfidf_model.save((output_dir / 'tfidf_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-17T09:54:23.630717Z","iopub.execute_input":"2024-10-17T09:54:23.631054Z","iopub.status.idle":"2024-10-17T09:54:23.636307Z","shell.execute_reply.started":"2024-10-17T09:54:23.631026Z","shell.execute_reply":"2024-10-17T09:54:23.635444Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport dask.dataframe as dd\n\nTOPIC_NUM = 100\n\ntmp_dir = Path('./')\nanswers_dir = Path(r\"./answers\")\nanswers_file = answers_dir/\"insiders.csv\"\ndataset_version = '5.2'\nassert(answers_file.is_file())","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.211957Z","iopub.status.idle":"2024-10-17T10:43:18.212420Z","shell.execute_reply.started":"2024-10-17T10:43:18.212192Z","shell.execute_reply":"2024-10-17T10:43:18.212214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://stackoverflow.com/questions/57531388/how-can-i-reduce-the-memory-of-a-pandas-dataframe\ndef reduce_mem_usage(df, ignore_cols = None ):\n    \"\"\" \n    iterate through all the columns of a dataframe and \n    modify the data type to reduce memory usage.        \n    \"\"\"\n    if ignore_cols is None:\n\t    ignore_cols = []\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        print(col, col_type)\n\n        if col in ignore_cols:\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max <                  np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max <                   np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max <                   np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max <                   np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max <                   np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max <                   np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n        print(\"\\tNew dtype:  \", df[col].dtype)\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(('Memory usage after optimization is: {:.2f}' \n                              'MB').format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \n                                             / start_mem))\n    \n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.213802Z","iopub.status.idle":"2024-10-17T10:43:18.214299Z","shell.execute_reply.started":"2024-10-17T10:43:18.214049Z","shell.execute_reply":"2024-10-17T10:43:18.214073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[4]:\n\n\npreprocessed_dir = Path(f'./CERT_output/')\nassert(preprocessed_dir.is_dir())","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.215703Z","iopub.status.idle":"2024-10-17T10:43:18.216254Z","shell.execute_reply.started":"2024-10-17T10:43:18.216008Z","shell.execute_reply":"2024-10-17T10:43:18.216034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[9]:\n\n\nprocessed_files = ['logon_preprocessed.csv', 'http_preprocessed.csv','device_preprocessed.csv',\n                  'file_preprocessed.csv']\ncolumns = ['id', 'date', 'user', 'is_work_time', 'subtype']\ntemp_df=[]\n\nfor file in processed_files:\n    temp_df.append(pd.read_csv(preprocessed_dir/file,usecols = columns))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.217865Z","iopub.status.idle":"2024-10-17T10:43:18.218307Z","shell.execute_reply.started":"2024-10-17T10:43:18.218077Z","shell.execute_reply":"2024-10-17T10:43:18.218099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[10]:\n\n\ndf = pd.concat(temp_df, axis=0)\ndf.isna().sum()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.219503Z","iopub.status.idle":"2024-10-17T10:43:18.219988Z","shell.execute_reply.started":"2024-10-17T10:43:18.219718Z","shell.execute_reply":"2024-10-17T10:43:18.219740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[11]:\n\n\nfor temp in temp_df:\n    del temp","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.221320Z","iopub.status.idle":"2024-10-17T10:43:18.221683Z","shell.execute_reply.started":"2024-10-17T10:43:18.221515Z","shell.execute_reply":"2024-10-17T10:43:18.221532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[12]:\n\n\ndf = reduce_mem_usage(df, ignore_cols=['id', 'date'])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.222524Z","iopub.status.idle":"2024-10-17T10:43:18.222897Z","shell.execute_reply.started":"2024-10-17T10:43:18.222691Z","shell.execute_reply":"2024-10-17T10:43:18.222708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[13]:\n\n\nsubtype_encoder = LabelEncoder()\ntype_encoder = LabelEncoder()\ndf.subtype = df.subtype.map(str)\ndf['subtype'] = subtype_encoder.fit_transform(df['subtype'])\n#df['type'] = type_encoder.fit_transform(df['type'])\ndf['type'] = type_encoder.fit_transform(df['subtype'])\n\n#df['action_id'] = df.is_usual_pc.astype(np.int8) * 100 + df.is_work_time.astype(np.int8) * 10 + df.subtype\ndf['action_id'] =  df.is_work_time.astype(np.int8) * 10 + df.subtype\ndf['date'] = pd.to_datetime(df.date, format='%Y/%m/%d %H:%M:%S')\n\ndf = df[['id', 'date', 'user', 'action_id']]\n\ndf.to_pickle(str(tmp_dir / \"df.pkl\"))\ndel df","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.224066Z","iopub.status.idle":"2024-10-17T10:43:18.224365Z","shell.execute_reply.started":"2024-10-17T10:43:18.224215Z","shell.execute_reply":"2024-10-17T10:43:18.224229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[14]:\n\n\ncontent_dir = Path(f'./CERT_output/')\ncontent_file = ['email_lda.csv', 'file_lda.csv', 'http_lda.csv']\ncontent_cols = ['id', 'content']\ntemp_df = []\n\nfor file in content_file:\n    df = pd.read_csv(content_dir/ file, usecols = content_cols)\n    df = reduce_mem_usage(df, ignore_cols=content_cols)\n    temp_df.append(df)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.225452Z","iopub.status.idle":"2024-10-17T10:43:18.225798Z","shell.execute_reply.started":"2024-10-17T10:43:18.225634Z","shell.execute_reply":"2024-10-17T10:43:18.225650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[16]:\n\n\n\ncontent_df = pd.concat(temp_df, axis=0)\n\nprint(content_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.227091Z","iopub.status.idle":"2024-10-17T10:43:18.227444Z","shell.execute_reply.started":"2024-10-17T10:43:18.227274Z","shell.execute_reply":"2024-10-17T10:43:18.227292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[17]:\n\n\nfor df in temp_df:\n    del df","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.228589Z","iopub.status.idle":"2024-10-17T10:43:18.228924Z","shell.execute_reply.started":"2024-10-17T10:43:18.228749Z","shell.execute_reply":"2024-10-17T10:43:18.228764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[18]:\n\n\ncontent_df = reduce_mem_usage(content_df, ignore_cols=['id', 'content'])\ncontent_df.to_csv(str(tmp_dir / 'content_df.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.229937Z","iopub.status.idle":"2024-10-17T10:43:18.230276Z","shell.execute_reply.started":"2024-10-17T10:43:18.230109Z","shell.execute_reply":"2024-10-17T10:43:18.230126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[5]:\n\n\ncontent_df = dd.read_csv(str(tmp_dir / 'content_df.csv'))    .set_index('id').drop('Unnamed: 0', axis=1)\ndf = pd.read_pickle(str(tmp_dir / \"df.pkl\"))\n\n# Merge the csv files.\ndf = dd.merge(content_df, df, how='inner', on=['id'])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.232186Z","iopub.status.idle":"2024-10-17T10:43:18.232505Z","shell.execute_reply.started":"2024-10-17T10:43:18.232345Z","shell.execute_reply":"2024-10-17T10:43:18.232361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[ ]:\n\n\n#df = df.reset_index().drop(['index', 'type'], axis=1)\ndf['day'] = df.date.dt.floor('D')\ndf.set_index('date')\n\naction_id_lists = df.groupby(['user', 'day'], sort=True)    ['action_id'].apply(list)\n\ncontent_lists = df.groupby(['user', 'day'], sort=True)    ['content'].apply(list)\n\naction_id_lists = action_id_lists.reset_index()\ncontent_lists = content_lists.reset_index()\n\ndf_merged = dd.merge(action_id_lists, content_lists, how='inner', on=['user', 'day'])\ndf_merged.to_csv(str(tmp_dir / \"merged_df.csv\"), index=False, single_file=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.234215Z","iopub.status.idle":"2024-10-17T10:43:18.234545Z","shell.execute_reply.started":"2024-10-17T10:43:18.234367Z","shell.execute_reply":"2024-10-17T10:43:18.234400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[ ]:\n\n\nmain_df = pd.read_csv(answers_file)\nmain_df = main_df[main_df['dataset'].astype(str) ==\n                  str(dataset_version)].drop(['dataset', 'details'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.236230Z","iopub.status.idle":"2024-10-17T10:43:18.236576Z","shell.execute_reply.started":"2024-10-17T10:43:18.236406Z","shell.execute_reply":"2024-10-17T10:43:18.236423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[ ]:\n\n\ndf = pd.read_csv(str(tmp_dir / \"merged_df.csv\"))\ndf = df.merge(main_df, left_on='user', right_on='user', how='left')\n#df = df.drop(['start', 'end', 'day', 'user'], axis=1)\ndf['day'] = pd.to_datetime(df.day, format='%Y-%m-%d ').dt.floor('D')\ndf['dayofweek']=df['day'].dt.dayofweek\ndf['malicious'] = (df['dayofweek'] >= 5)\ndf = df.drop(['day','user'],axis=1)\ndf = df.dropna()\ndf.to_csv(str(tmp_dir / 'merged_answers_df.csv'), index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.238098Z","iopub.status.idle":"2024-10-17T10:43:18.238444Z","shell.execute_reply.started":"2024-10-17T10:43:18.238274Z","shell.execute_reply":"2024-10-17T10:43:18.238291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In[ ]:\n\n\ndf = pd.read_csv(str(tmp_dir / 'merged_answers_df.csv'),)\ndf = reduce_mem_usage(df, ignore_cols=['action_id', 'content'])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.239786Z","iopub.status.idle":"2024-10-17T10:43:18.240153Z","shell.execute_reply.started":"2024-10-17T10:43:18.239978Z","shell.execute_reply":"2024-10-17T10:43:18.240001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nf = open('./dataset.csv','w',newline='', encoding='utf-8')\nwriter = csv.writer(f)\n# In[ ]:\n\ndf = df.dropna()\nimport ast\nfrom scipy.sparse import csc_matrix\n\nfor idx, row in df.iterrows():\n    content = ast.literal_eval(row.content)\n    day=row.dayofweek\n    scenario=row.scenario\n    action=ast.literal_eval(row.action_id)\n    mal=row.malicious\n    yl=len(content)\n    for iv in range(yl):\n        y=ast.literal_eval(content[iv])\n        ixtt=len(y)\n        for ix in range(ixtt):\n            ct=y[ix][0]\n            prob=y[ix][1]\n            print('Values ', ct,',',prob,',',action[iv],',',scenario,',',mal)\n            data=[ct,prob,action[iv],scenario,day,mal]\n            writer.writerow(data)\n\n\nf.close()            ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T10:43:18.241446Z","iopub.status.idle":"2024-10-17T10:43:18.241749Z","shell.execute_reply.started":"2024-10-17T10:43:18.241598Z","shell.execute_reply":"2024-10-17T10:43:18.241613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport re\nfrom tqdm import tqdm\nfrom math import ceil\nfrom itertools import chain\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:15:32.942234Z","iopub.execute_input":"2024-10-19T19:15:32.942594Z","iopub.status.idle":"2024-10-19T19:15:32.948209Z","shell.execute_reply.started":"2024-10-19T19:15:32.942558Z","shell.execute_reply":"2024-10-19T19:15:32.947332Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"answers_dir = Path(r\"/kaggle/input/answers/answers\")\ndataset_dir = Path(r\"/kaggle/input/cert-final/dataset\")\n\nmain_answers_file = answers_dir / \"insiders.csv\"\n\nassert(answers_dir.is_dir())\nassert(dataset_dir.is_dir())\nassert(main_answers_file.is_file())\n\noutput_dir = Path(r'/kaggle/working/')\nassert(output_dir.is_dir())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:16:29.525527Z","iopub.execute_input":"2024-10-19T19:16:29.526202Z","iopub.status.idle":"2024-10-19T19:16:29.536724Z","shell.execute_reply.started":"2024-10-19T19:16:29.526164Z","shell.execute_reply":"2024-10-19T19:16:29.535802Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"device_cols = next(pd.read_csv(dataset_dir / 'device.csv', chunksize=10)).columns\nemail_cols = next(pd.read_csv(dataset_dir / 'email.csv', chunksize=10)).columns\nfile_cols = next(pd.read_csv(dataset_dir / 'file.csv', chunksize=10)).columns\nhttp_cols = next(pd.read_csv(dataset_dir / 'http.csv', chunksize=10)).columns\nlogon_cols = next(pd.read_csv(dataset_dir / 'logon.csv', chunksize=10)).columns","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:16:39.443061Z","iopub.execute_input":"2024-10-19T19:16:39.443393Z","iopub.status.idle":"2024-10-19T19:16:39.510760Z","shell.execute_reply.started":"2024-10-19T19:16:39.443366Z","shell.execute_reply":"2024-10-19T19:16:39.509985Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def count_file_lines(file):\n    with open(file) as f:\n        for count, _ in enumerate(f):\n            pass\n    return count\n\ndef collect_vocabulary(csv_name, chunk_size=100000):\n    \n    line_count = count_file_lines(dataset_dir / f'{csv_name}.csv')\n    \n    df_iter = pd.read_csv(dataset_dir / f'{csv_name}.csv', usecols=['date', 'user', 'content'], chunksize=chunk_size)\n    \n    result_set = set()\n\n    for df in tqdm(df_iter, total=(ceil(line_count / chunk_size))):\n        df['content'] = df['content'].str.lower().str.split()\n        result_set = result_set.union(*map(set, df['content']))\n        \n    return result_set\n\n# email_set = collect_vocabulary('email', chunk_size=500000)\n# file_set = collect_vocabulary('file', chunk_size=500000)\n# http_set = collect_vocabulary('http', chunk_size=500000)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:16:51.689220Z","iopub.execute_input":"2024-10-19T19:16:51.689580Z","iopub.status.idle":"2024-10-19T19:16:51.696952Z","shell.execute_reply.started":"2024-10-19T19:16:51.689550Z","shell.execute_reply":"2024-10-19T19:16:51.696067Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from gensim.models import TfidfModel, nmf\nfrom gensim.corpora import Dictionary\nfrom gensim.models.ldamulticore import LdaModel\n\ndef chunk_iterator(filename, chunk_size=10000):\n    for chunk in tqdm(pd.read_csv(filename, chunksize=chunk_size)):\n        for document in chunk['content'].str.lower().str.split().values:\n            yield document\n            \ndef bow_chunk_iterator(filenames, dictionary, chunk_size=10000):\n    for filename in filenames:\n        print(filename)\n        for chunk in tqdm(pd.read_csv(filename, chunksize=chunk_size)):\n            for document in chunk['content'].str.lower().str.split().values:\n                yield dictionary.doc2bow(document)\n                \ndef tfidf_chunk_iterator(filenames, dictionary, tfidf, chunk_size=10000):\n    for filename in filenames:\n        print(filename)\n        for chunk in tqdm(pd.read_csv(filename, chunksize=chunk_size)):\n            for document in chunk['content'].str.lower().str.split().values:\n                yield tfidf[dictionary.doc2bow(document)]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:17:01.140863Z","iopub.execute_input":"2024-10-19T19:17:01.141672Z","iopub.status.idle":"2024-10-19T19:17:09.210411Z","shell.execute_reply.started":"2024-10-19T19:17:01.141639Z","shell.execute_reply":"2024-10-19T19:17:09.209623Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"df_dict = Dictionary(chunk_iterator(dataset_dir / 'email.csv'))\ndf_dict.add_documents(chunk_iterator(dataset_dir / 'file.csv'))\ndf_dict.add_documents(chunk_iterator(dataset_dir / 'http.csv'))\n\ndf_dict.save((output_dir / 'content_dictionary.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:17:24.044622Z","iopub.execute_input":"2024-10-19T19:17:24.045217Z","iopub.status.idle":"2024-10-19T19:17:24.341407Z","shell.execute_reply.started":"2024-10-19T19:17:24.045182Z","shell.execute_reply":"2024-10-19T19:17:24.340124Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"1it [00:00, 10.72it/s]\n1it [00:00, 13.10it/s]\n1it [00:00, 11.93it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_dict = Dictionary.load((output_dir / 'content_dictionary.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:17:33.326233Z","iopub.execute_input":"2024-10-19T19:17:33.326603Z","iopub.status.idle":"2024-10-19T19:17:33.348644Z","shell.execute_reply.started":"2024-10-19T19:17:33.326571Z","shell.execute_reply":"2024-10-19T19:17:33.347869Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfModel(\n    bow_chunk_iterator([\n        dataset_dir / 'email.csv',\n        dataset_dir / 'file.csv',\n        dataset_dir / 'http.csv'\n    ], df_dict))\n\ntfidf.save((output_dir / 'tfidf_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:20:09.853609Z","iopub.execute_input":"2024-10-19T19:20:09.854468Z","iopub.status.idle":"2024-10-19T19:20:10.212632Z","shell.execute_reply.started":"2024-10-19T19:20:09.854434Z","shell.execute_reply":"2024-10-19T19:20:10.211873Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"/kaggle/input/cert-final/dataset/email.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00, 18.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cert-final/dataset/file.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00, 19.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cert-final/dataset/http.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00, 17.72it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"tfidf = TfidfModel.load((output_dir / 'tfidf_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:20:13.895850Z","iopub.execute_input":"2024-10-19T19:20:13.896667Z","iopub.status.idle":"2024-10-19T19:20:13.921860Z","shell.execute_reply.started":"2024-10-19T19:20:13.896636Z","shell.execute_reply":"2024-10-19T19:20:13.921176Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"nmf_model = nmf.Nmf(\n    tfidf_chunk_iterator([\n        dataset_dir / 'email.csv',\n        dataset_dir / 'file.csv',\n        dataset_dir / 'http.csv'\n    ], df_dict, tfidf,\n    ),\n    num_topics=100\n)\n\nnmf_model.save((output_dir / 'nmf_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:20:29.455665Z","iopub.execute_input":"2024-10-19T19:20:29.456360Z","iopub.status.idle":"2024-10-19T19:20:29.906943Z","shell.execute_reply.started":"2024-10-19T19:20:29.456326Z","shell.execute_reply":"2024-10-19T19:20:29.906114Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"/kaggle/input/cert-final/dataset/email.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  7.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cert-final/dataset/file.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  7.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cert-final/dataset/http.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  7.21it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"lda_model = LdaModel(\n    tfidf_chunk_iterator([\n        dataset_dir / 'email.csv',\n        dataset_dir / 'file.csv',\n        dataset_dir / 'http.csv'\n    ], df_dict, tfidf,\n    ),\n    num_topics=100\n)\n\nlda_model.save((output_dir / 'lda_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:21:05.531716Z","iopub.execute_input":"2024-10-19T19:21:05.532630Z","iopub.status.idle":"2024-10-19T19:21:06.230994Z","shell.execute_reply.started":"2024-10-19T19:21:05.532595Z","shell.execute_reply":"2024-10-19T19:21:06.230203Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"/kaggle/input/cert-final/dataset/email.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  7.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cert-final/dataset/file.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  7.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/cert-final/dataset/http.csv\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  7.17it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"lda_model = LdaModel.load((output_dir / 'lda_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:21:16.717109Z","iopub.execute_input":"2024-10-19T19:21:16.717484Z","iopub.status.idle":"2024-10-19T19:21:16.730807Z","shell.execute_reply.started":"2024-10-19T19:21:16.717454Z","shell.execute_reply":"2024-10-19T19:21:16.730022Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def process_content(filename, dictionary, model, chunk_size = 10000, postfix='_lda_content'):\n    out_file = output_dir / (filename.stem + postfix + '.csv')\n    assert(not out_file.is_file())\n    for chunk in tqdm(pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size)):\n        chunk['content'] = chunk['content']\\\n            .str.lower()\\\n            .str.split()\\\n            .apply(lambda doc: model[dictionary.doc2bow(doc)])\n\n        chunk.to_csv(out_file, mode='a')","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:21:26.430690Z","iopub.execute_input":"2024-10-19T19:21:26.431035Z","iopub.status.idle":"2024-10-19T19:21:26.437613Z","shell.execute_reply.started":"2024-10-19T19:21:26.431008Z","shell.execute_reply":"2024-10-19T19:21:26.436625Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"process_content(dataset_dir / '/kaggle/input/cert-final/dataset/email.csv', df_dict, lda_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:21:45.960671Z","iopub.execute_input":"2024-10-19T19:21:45.961495Z","iopub.status.idle":"2024-10-19T19:21:48.026868Z","shell.execute_reply.started":"2024-10-19T19:21:45.961458Z","shell.execute_reply":"2024-10-19T19:21:48.025885Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"1it [00:02,  2.05s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"process_content(dataset_dir / '/kaggle/input/cert-final/dataset/file.csv', df_dict, lda_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:22:07.559096Z","iopub.execute_input":"2024-10-19T19:22:07.559922Z","iopub.status.idle":"2024-10-19T19:22:09.523053Z","shell.execute_reply.started":"2024-10-19T19:22:07.559863Z","shell.execute_reply":"2024-10-19T19:22:09.521852Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"1it [00:01,  1.95s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# credit to https://stackoverflow.com/a/53135031\n\ndef parallelize(data, func, num_of_processes=8):\n    data_split = np.array_split(data, num_of_processes)\n    pool = Pool(num_of_processes)\n    data = pd.concat(pool.map(func, data_split))\n    pool.close()\n    pool.join()\n    return data\n\ndef run_on_subset(func, data_subset):\n    return data_subset.apply(func, axis=1)\n\ndef parallelize_on_rows(data, func, num_of_processes=8):\n    return parallelize(data, partial(run_on_subset, func), num_of_processes)\n\ndef process_content_parallelized(filename, dictionary, model, chunk_size = 10000, postfix='_lda_content'):\n    out_file = output_dir / (filename.stem + postfix + '.csv')\n    assert(not out_file.is_file())\n    for chunk in tqdm(pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size)):\n    \n        chunk['content'] = parallelize_on_rows(\n            chunk['content'].str.lower().str.split()\n            , lambda doc: model[dictionary.doc2bow(doc)])\n\n        chunk.to_csv(out_file, mode='a')","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:22:20.390467Z","iopub.execute_input":"2024-10-19T19:22:20.391264Z","iopub.status.idle":"2024-10-19T19:22:20.399978Z","shell.execute_reply.started":"2024-10-19T19:22:20.391232Z","shell.execute_reply":"2024-10-19T19:22:20.398950Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"process_content(dataset_dir / 'http.csv', df_dict, lda_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:25:24.466451Z","iopub.execute_input":"2024-10-19T19:25:24.467304Z","iopub.status.idle":"2024-10-19T19:25:26.446502Z","shell.execute_reply.started":"2024-10-19T19:25:24.467271Z","shell.execute_reply":"2024-10-19T19:25:26.445178Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"1it [00:01,  1.97s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(df.dtypes)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:52:51.976031Z","iopub.execute_input":"2024-10-21T06:52:51.976812Z","iopub.status.idle":"2024-10-21T06:52:51.982150Z","shell.execute_reply.started":"2024-10-21T06:52:51.976777Z","shell.execute_reply":"2024-10-21T06:52:51.981136Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"date            object\nuser            object\nis_usual_pc       bool\nis_work_time      bool\nsubtype         object\ntype            object\ndtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert 'subtype' and 'type' to consistent types\ndf['subtype'] = df['subtype'].astype(str)  # Ensure 'subtype' is a string\ndf['type'] = df['type'].astype(str)        # Ensure 'type' is also a string (or change to the desired type)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T06:52:53.958781Z","iopub.execute_input":"2024-10-21T06:52:53.959510Z","iopub.status.idle":"2024-10-21T06:52:53.976067Z","shell.execute_reply.started":"2024-10-21T06:52:53.959472Z","shell.execute_reply":"2024-10-21T06:52:53.975068Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom pathlib import Path\nimport re\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport seaborn as sns\nimport pickle\nimport gc\nimport dask\nimport dask.dataframe as dd\nfrom scipy.sparse import csr_matrix as sparse_matrix\nfrom IPython.core.debugger import Pdb\n\n# Initialize debugger\nipdb = Pdb()\n\n# Enable tqdm for progress bars\ntqdm.pandas()\n\n# Dask progress bar setup\nfrom dask.diagnostics import ProgressBar\nProgressBar().register()\n\n# Define dataset version\ndataset_version = '4.2'\n\n# Define paths\npreprocessed_dir = Path(f'/kaggle/input/preprocessedfiles')\nassert preprocessed_dir.is_dir(), \"Preprocessed directory not found\"\n\ntmp_dir = Path('/kaggle/working/tmp')\nassert tmp_dir.is_dir(), \"Temporary directory not found\"\ndask.config.set(temporary_directory=tmp_dir.as_posix())\n\n# Initial processing: Load datasets\nlogon_df = dd.read_csv(preprocessed_dir / 'logon_preprocessed (3).csv', \n                       usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nlogon_df['type'] = 'logon'\nprint('logon')\n\nhttp_df = dd.read_csv(preprocessed_dir / 'http_preprocessed (1).csv', \n                      usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nhttp_df['type'] = 'http'\nhttp_df.subtype = http_df.subtype.map(int)  # Ensure correct mapping for http\nprint('http')\n\ndevice_df = dd.read_csv(preprocessed_dir / 'device_preprocessed.csv', \n                        usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\ndevice_df['type'] = 'device'\nprint('device')\n\nemail_df = dd.read_csv(preprocessed_dir / 'email_preprocessed.csv', \n                       usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nemail_df['type'] = 'email'\nemail_df.subtype = email_df.subtype.map({True: 'external', False: 'internal'}).astype(str)\nprint('email')\n\nfile_df = dd.read_csv(preprocessed_dir / 'file_preprocessed.csv', \n                      usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nfile_df['type'] = 'file'\nprint('file')\n\n# Fix mixed types in the 'subtype' column by converting everything to strings\nlogon_df['subtype'] = logon_df['subtype'].astype(str)\nhttp_df['subtype'] = http_df['subtype'].astype(str)  # Ensure consistent type conversion\ndevice_df['subtype'] = device_df['subtype'].astype(str)\nfile_df['subtype'] = file_df['subtype'].astype(str)\n\n# Persist the loaded datasets\nlogon_df, http_df, device_df, email_df, file_df = dask.persist(logon_df, http_df, device_df, email_df, file_df)\n\n# Concatenate datasets\ndf = dd.concat([logon_df, http_df, device_df, email_df, file_df], axis=0)\ndel logon_df, http_df, device_df, email_df, file_df\ngc.collect()\n\ndf = df.persist()\n\n# Categorize columns\ndf = df.categorize(columns=['subtype', 'type', 'is_usual_pc', 'is_work_time', 'user'])\n\n# Convert the categorical columns to their respective codes\ndf['type'] = df['type'].cat.codes\ndf['subtype'] = df['subtype'].cat.codes\ndf['is_usual_pc'] = df['is_usual_pc'].cat.codes\ndf['is_work_time'] = df['is_work_time'].cat.codes\n\ndf = df.persist()\n\n# Construct combined categorical column\ndf['action_id'] = df.is_usual_pc * 32 + df.is_work_time * 16 + df.subtype\ndf = df.persist()\n\n# Process datetime columns using corrected format\ndf['date'] = dd.to_datetime(df.date, format='%Y-%m-%d %H:%M:%S')  # Corrected format here\ndf['day'] = df.date.dt.floor('D')\ndf['day_minutes'] = df['date'].dt.hour * 60 + df['date'].dt.minute\n\ndf = df[['day', 'user', 'action_id', 'day_minutes']]\ndf = df.persist()\n\n# Check memory usage\nmemory_usage = df.memory_usage().sum().compute() / 1024**2\nprint(f'Memory usage: {memory_usage} MB')\n\n# Check dataset shape\nprint(f'Number of rows: {df.shape[0].compute()}')\n\n# Save the dataframe as parquet\ndf.to_parquet(tmp_dir / 'df.parquet')\n\n# Grouping operations\ndf = dd.read_parquet(tmp_dir / 'df.parquet')\naction_id_lists = df.groupby('user')\\\n    .apply(pd.DataFrame.sort_values, ['day', 'day_minutes'])\\\n    .reset_index(drop=True)\\\n    .groupby(['user', 'day'])[['action_id', 'day_minutes']].apply(\n        lambda x: [list(x['action_id']), list(x['day_minutes'])],\n        meta=pd.Series(dtype=object)\n    ).compute()\n\n# Clean the grouped data\naction_id_lists = action_id_lists.dropna().progress_apply(pd.Series).reset_index()\naction_id_lists.columns = ['user', 'day', 'action_id', 'day_minutes']\n\n# Save the aggregated data\naction_id_lists.to_parquet(tmp_dir / 'df_aggregated.parquet')\n\n# Validate the length of action_id and day_minutes\nassert (action_id_lists['action_id'].apply(len) == action_id_lists['day_minutes'].apply(len)).all()\n\n# Check if day_minutes lists are sorted\nfor ls in tqdm(action_id_lists.day_minutes):\n    assert all(ls[i] <= ls[i+1] for i in range(len(ls)-1))\n\n# Load insider data and merge with the main dataset\nanswers_dir = Path(r\"/kaggle/input/answers\")\nassert answers_dir.is_dir(), \"Answers directory not found\"\n\ninsider_files = list(answers_dir.glob(f'**/r{dataset_version}*.csv'))\n\nanswers_df_ls = []\nfor filename in insider_files:\n    answers_df = pd.read_csv(filename, names=list(range(13)))\n    answers_df_ls.append(answers_df)\n\nanswers_df = pd.concat(answers_df_ls, axis=0, ignore_index=True)\n\n# Load main insider dataset\nanswers_csv = Path(\"/kaggle/input/answers/answers/insiders.csv\")\nassert answers_csv.is_file(), \"Insiders CSV not found\"\n\nmain_df = pd.read_csv(answers_csv)\nmain_df = main_df[main_df['dataset'].astype(str) == str(dataset_version)]\\\n    .drop(['dataset', 'details'], axis=1)\nmain_df['start'] = pd.to_datetime(main_df['start'], format='%m/%d/%Y %H:%M:%S')\nmain_df['end'] = pd.to_datetime(main_df['end'], format='%m/%d/%Y %H:%M:%S')\n\n# Merge with action_id data\ndf = dd.merge(action_id_lists, main_df, on='user', how='left')\ndf['malicious'] = (df.day >= df.start) & (df.day <= df.end)\ndf = df.drop(['start', 'end', 'scenario'], axis=1)\n\n# Save final data as parquet\ndf.to_parquet(tmp_dir / f'final_{dataset_version}.parquet')\n\n# Check the proportion of malicious events\nmalicious_mean = df.malicious.mean()  # No need for .compute() here\nprint(f'Malicious rate: {malicious_mean}')\n\n\n# Check dataset size\nprint(f'Final dataset size: {len(df)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T08:28:31.024768Z","iopub.execute_input":"2024-10-21T08:28:31.025265Z","iopub.status.idle":"2024-10-21T08:40:12.879669Z","shell.execute_reply.started":"2024-10-21T08:28:31.025230Z","shell.execute_reply":"2024-10-21T08:40:12.878250Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"logon\nhttp\ndevice\nemail\nfile\n[########################################] | 100% Completed | 24.09 s\n[########################################] | 100% Completed | 24.17 s\n[########################################] | 100% Completed | 24.19 s\n[########################################] | 100% Completed | 24.22 s\n[########################################] | 100% Completed | 24.32 s\n[########################################] | 100% Completed | 24.37 s\n[########################################] | 100% Completed | 24.39 s\n[########################################] | 100% Completed | 24.49 s\n[########################################] | 100% Completed | 24.59 s\n[########################################] | 100% Completed | 24.62 s\n[########################################] | 100% Completed | 24.69 s\n[########################################] | 100% Completed | 24.69 s\n[########################################] | 100% Completed | 24.62 s\n[########################################] | 100% Completed | 103.18 ms\n[########################################] | 100% Completed | 201.84 ms\n[########################################] | 100% Completed | 301.74 ms\n[########################################] | 100% Completed | 304.60 ms\n[########################################] | 100% Completed | 404.23 ms\n[########################################] | 100% Completed | 502.74 ms\n[########################################] | 100% Completed | 602.04 ms\n[########################################] | 100% Completed | 604.83 ms\n[########################################] | 100% Completed | 703.74 ms\n[########################################] | 100% Completed | 802.91 ms\n[########################################] | 100% Completed | 902.58 ms\n[########################################] | 100% Completed | 904.42 ms\n[########################################] | 100% Completed | 1.00 s\n[########################################] | 100% Completed | 10.79 s\n[########################################] | 100% Completed | 10.89 s\n[########################################] | 100% Completed | 10.99 s\n[########################################] | 100% Completed | 11.09 s\n[########################################] | 100% Completed | 11.09 s\n[########################################] | 100% Completed | 11.19 s\n[########################################] | 100% Completed | 11.29 s\n[########################################] | 100% Completed | 11.39 s\n[########################################] | 100% Completed | 11.39 s\n[########################################] | 100% Completed | 11.49 s\n[########################################] | 100% Completed | 11.49 s\n[########################################] | 100% Completed | 11.59 s\n[########################################] | 100% Completed | 11.69 s\n[########################################] | 100% Completed | 10.52 s\n[########################################] | 100% Completed | 10.62 s\n[########################################] | 100% Completed | 10.72 s\n[########################################] | 100% Completed | 10.82 s\n[########################################] | 100% Completed | 10.82 s\n[########################################] | 100% Completed | 10.92 s\n[########################################] | 100% Completed | 11.02 s\n[########################################] | 100% Completed | 11.12 s\n[########################################] | 100% Completed | 11.12 s\n[########################################] | 100% Completed | 11.22 s\n[########################################] | 100% Completed | 11.32 s\n[########################################] | 100% Completed | 11.42 s\n[########################################] | 100% Completed | 11.52 s\n[########################################] | 100% Completed | 104.07 ms\n[########################################] | 100% Completed | 205.48 ms\n[########################################] | 100% Completed | 304.23 ms\n[########################################] | 100% Completed | 404.71 ms\n[########################################] | 100% Completed | 503.69 ms\n[########################################] | 100% Completed | 603.50 ms\n[########################################] | 100% Completed | 704.63 ms\n[########################################] | 100% Completed | 803.07 ms\n[########################################] | 100% Completed | 905.46 ms\n[########################################] | 100% Completed | 1.01 s\n[########################################] | 100% Completed | 1.11 s\n[########################################] | 100% Completed | 1.21 s\n[########################################] | 100% Completed | 1.31 s\n[########################################] | 100% Completed | 12.32 s\n[########################################] | 100% Completed | 12.32 s\n[########################################] | 100% Completed | 12.42 s\n[########################################] | 100% Completed | 12.42 s\n[########################################] | 100% Completed | 12.52 s\n[########################################] | 100% Completed | 12.62 s\n[########################################] | 100% Completed | 12.62 s\n[########################################] | 100% Completed | 12.72 s\n[########################################] | 100% Completed | 12.73 s\n[########################################] | 100% Completed | 12.82 s\n[########################################] | 100% Completed | 12.83 s\n[########################################] | 100% Completed | 12.92 s\n[########################################] | 100% Completed | 13.01 s\n[########################################] | 100% Completed | 207.51 ms\n[########################################] | 100% Completed | 302.88 ms\n[########################################] | 100% Completed | 403.97 ms\n[########################################] | 100% Completed | 404.91 ms\n[########################################] | 100% Completed | 506.01 ms\n[########################################] | 100% Completed | 604.34 ms\n[########################################] | 100% Completed | 605.70 ms\n[########################################] | 100% Completed | 705.44 ms\n[########################################] | 100% Completed | 804.88 ms\n[########################################] | 100% Completed | 904.22 ms\n[########################################] | 100% Completed | 910.92 ms\n[########################################] | 100% Completed | 1.01 s\n[########################################] | 100% Completed | 1.11 s\nMemory usage: 351.64094829559326 MB\n[########################################] | 100% Completed | 104.90 ms\n[########################################] | 100% Completed | 202.69 ms\n[########################################] | 100% Completed | 302.27 ms\n[########################################] | 100% Completed | 303.58 ms\n[########################################] | 100% Completed | 305.88 ms\n[########################################] | 100% Completed | 405.61 ms\n[########################################] | 100% Completed | 406.39 ms\n[########################################] | 100% Completed | 403.57 ms\n[########################################] | 100% Completed | 502.75 ms\n[########################################] | 100% Completed | 503.06 ms\n[########################################] | 100% Completed | 602.28 ms\n[########################################] | 100% Completed | 604.11 ms\n[########################################] | 100% Completed | 702.87 ms\nNumber of rows: 24406269\n[########################################] | 100% Completed | 1.74 sms\n[########################################] | 100% Completed | 1.83 s\n[########################################] | 100% Completed | 1.84 s\n[########################################] | 100% Completed | 1.85 s\n[########################################] | 100% Completed | 1.85 s\n[########################################] | 100% Completed | 1.94 s\n[########################################] | 100% Completed | 1.96 s\n[########################################] | 100% Completed | 2.04 s\n[########################################] | 100% Completed | 2.04 s\n[########################################] | 100% Completed | 2.05 s\n[########################################] | 100% Completed | 2.14 s\n[########################################] | 100% Completed | 2.24 s\n[########################################] | 100% Completed | 2.25 s\n[                                        ] | 0% Completed | 107.47 ms","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/3526336632.py:117: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n  .apply(pd.DataFrame.sort_values, ['day', 'day_minutes'])\\\n","output_type":"stream"},{"name":"stdout","text":"[########################################] | 100% Completed | 361.36 s\n[########################################] | 100% Completed | 361.36 s\n[########################################] | 100% Completed | 361.46 s\n[########################################] | 100% Completed | 361.56 s\n[########################################] | 100% Completed | 361.66 s\n[########################################] | 100% Completed | 361.66 s\n[########################################] | 100% Completed | 361.76 s\n[########################################] | 100% Completed | 361.85 s\n[########################################] | 100% Completed | 361.86 s\n[########################################] | 100% Completed | 361.86 s\n[########################################] | 100% Completed | 361.95 s\n[########################################] | 100% Completed | 361.95 s\n[########################################] | 100% Completed | 362.05 s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1394010/1394010 [04:04<00:00, 5709.96it/s] \n100%|██████████| 1394010/1394010 [00:06<00:00, 218744.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Malicious rate: 0.0\nFinal dataset size: 1394010\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom pathlib import Path\nimport re\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport seaborn as sns\nimport pickle\nimport gc\nimport dask\nimport dask.dataframe as dd\nfrom scipy.sparse import csr_matrix as sparse_matrix\nfrom IPython.core.debugger import Pdb\n\n# Initialize debugger\nipdb = Pdb()\n\n# Enable tqdm for progress bars\ntqdm.pandas()\n\n# Dask progress bar setup\nfrom dask.diagnostics import ProgressBar\nProgressBar().register()\n\n# Define dataset version\ndataset_version = '4.2'\n\n# Define paths\npreprocessed_dir = Path(f'/kaggle/input/preprocessedfiles')\nassert preprocessed_dir.is_dir(), \"Preprocessed directory not found\"\n\ntmp_dir = Path('/kaggle/working/tmp')\nassert tmp_dir.is_dir(), \"Temporary directory not found\"\ndask.config.set(temporary_directory=tmp_dir.as_posix())\n\n# Initial processing: Load datasets\nlogon_df = dd.read_csv(preprocessed_dir / 'logon_preprocessed (3).csv', \n                       usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nlogon_df['type'] = 'logon'\nprint('logon')\n\nhttp_df = dd.read_csv(preprocessed_dir / 'http_preprocessed (1).csv', \n                      usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nhttp_df['type'] = 'http'\nhttp_df.subtype = http_df.subtype.map(int)  # Ensure correct mapping for http\nprint('http')\n\ndevice_df = dd.read_csv(preprocessed_dir / 'device_preprocessed.csv', \n                        usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\ndevice_df['type'] = 'device'\nprint('device')\n\nemail_df = dd.read_csv(preprocessed_dir / 'email_preprocessed.csv', \n                       usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nemail_df['type'] = 'email'\nemail_df.subtype = email_df.subtype.map({True: 'external', False: 'internal'}).astype(str)\nprint('email')\n\nfile_df = dd.read_csv(preprocessed_dir / 'file_preprocessed.csv', \n                      usecols=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype'])\nfile_df['type'] = 'file'\nprint('file')\n\n# Fix mixed types in the 'subtype' column by converting everything to strings\nlogon_df['subtype'] = logon_df['subtype'].astype(str)\nhttp_df['subtype'] = http_df['subtype'].astype(str)  # Ensure consistent type conversion\ndevice_df['subtype'] = device_df['subtype'].astype(str)\nfile_df['subtype'] = file_df['subtype'].astype(str)\n\n# Persist the loaded datasets\nlogon_df, http_df, device_df, email_df, file_df = dask.persist(logon_df, http_df, device_df, email_df, file_df)\n\n# Concatenate datasets\ndf = dd.concat([logon_df, http_df, device_df, email_df, file_df], axis=0)\ndel logon_df, http_df, device_df, email_df, file_df\ngc.collect()\n\n# Save the combined dataframe as a CSV\ndf.to_csv(tmp_dir / 'combined_data.csv', index=False)\n\ndf = df.persist()\n\n# Categorize columns\ndf = df.categorize(columns=['subtype', 'type', 'is_usual_pc', 'is_work_time', 'user'])\n\n# Convert the categorical columns to their respective codes\ndf['type'] = df['type'].cat.codes\ndf['subtype'] = df['subtype'].cat.codes\ndf['is_usual_pc'] = df['is_usual_pc'].cat.codes\ndf['is_work_time'] = df['is_work_time'].cat.codes\n\ndf = df.persist()\n\n# Construct combined categorical column\ndf['action_id'] = df.is_usual_pc * 32 + df.is_work_time * 16 + df.subtype\ndf = df.persist()\n\n# Process datetime columns using corrected format\ndf['date'] = dd.to_datetime(df.date, format='%Y-%m-%d %H:%M:%S')  # Corrected format here\ndf['day'] = df.date.dt.floor('D')\ndf['day_minutes'] = df['date'].dt.hour * 60 + df['date'].dt.minute\n\ndf = df[['day', 'user', 'action_id', 'day_minutes']]\ndf = df.persist()\n\n# Check memory usage\nmemory_usage = df.memory_usage().sum().compute() / 1024**2\nprint(f'Memory usage: {memory_usage} MB')\n\n# Check dataset shape\nprint(f'Number of rows: {df.shape[0].compute()}')\n\n# Save the dataframe as parquet\ndf.to_parquet(tmp_dir / 'df.parquet')\n\n# Grouping operations\ndf = dd.read_parquet(tmp_dir / 'df.parquet')\naction_id_lists = df.groupby('user')\\\n    .apply(pd.DataFrame.sort_values, ['day', 'day_minutes'])\\\n    .reset_index(drop=True)\\\n    .groupby(['user', 'day'])[['action_id', 'day_minutes']].apply(\n        lambda x: [list(x['action_id']), list(x['day_minutes'])],\n        meta=pd.Series(dtype=object)\n    ).compute()\n\n# Clean the grouped data\naction_id_lists = action_id_lists.dropna().progress_apply(pd.Series).reset_index()\naction_id_lists.columns = ['user', 'day', 'action_id', 'day_minutes']\n\n# Save the aggregated data\naction_id_lists.to_parquet(tmp_dir / 'df_aggregated.parquet')\n\n# Validate the length of action_id and day_minutes\nassert (action_id_lists['action_id'].apply(len) == action_id_lists['day_minutes'].apply(len)).all()\n\n# Check if day_minutes lists are sorted\nfor ls in tqdm(action_id_lists.day_minutes):\n    assert all(ls[i] <= ls[i+1] for i in range(len(ls)-1))\n\n# Load insider data and merge with the main dataset\nanswers_dir = Path(r\"/kaggle/input/answers\")\nassert answers_dir.is_dir(), \"Answers directory not found\"\n\ninsider_files = list(answers_dir.glob(f'**/r{dataset_version}*.csv'))\n\nanswers_df_ls = []\nfor filename in insider_files:\n    answers_df = pd.read_csv(filename, names=list(range(13)))\n    answers_df_ls.append(answers_df)\n\nanswers_df = pd.concat(answers_df_ls, axis=0, ignore_index=True)\n\n# Load main insider dataset\nanswers_csv = Path(\"/kaggle/input/answers/answers/insiders.csv\")\nassert answers_csv.is_file(), \"Insiders CSV not found\"\n\nmain_df = pd.read_csv(answers_csv)\nmain_df = main_df[main_df['dataset'].astype(str) == str(dataset_version)]\\\n    .drop(['dataset', 'details'], axis=1)\nmain_df['start'] = pd.to_datetime(main_df['start'], format='%m/%d/%Y %H:%M:%S')\nmain_df['end'] = pd.to_datetime(main_df['end'], format='%m/%d/%Y %H:%M:%S')\n\n# Merge with action_id data\ndf = dd.merge(action_id_lists, main_df, on='user', how='left')\ndf['malicious'] = (df.day >= df.start) & (df.day <= df.end)\ndf = df.drop(['start', 'end', 'scenario'], axis=1)\n\n# Save final data as parquet\ndf.to_parquet(tmp_dir / f'final_{dataset_version}.parquet')\n\n# Check the proportion of malicious events\nmalicious_mean = df.malicious.mean()  # No need for .compute() here\nprint(f'Malicious rate: {malicious_mean}')\n\n# Check dataset size\nprint(f'Final dataset size: {len(df)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T08:51:07.656109Z","iopub.execute_input":"2024-10-21T08:51:07.656820Z","iopub.status.idle":"2024-10-21T09:05:06.832564Z","shell.execute_reply.started":"2024-10-21T08:51:07.656782Z","shell.execute_reply":"2024-10-21T09:05:06.831402Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"logon\nhttp\ndevice\nemail\nfile\n[########################################] | 100% Completed | 24.72 s\n[########################################] | 100% Completed | 24.72 s\n[########################################] | 100% Completed | 24.82 s\n[########################################] | 100% Completed | 24.92 s\n[########################################] | 100% Completed | 24.95 s\n[########################################] | 100% Completed | 25.01 s\n[########################################] | 100% Completed | 25.11 s\n[########################################] | 100% Completed | 25.11 s\n[########################################] | 100% Completed | 25.21 s\n[########################################] | 100% Completed | 25.22 s\n[########################################] | 100% Completed | 25.31 s\n[########################################] | 100% Completed | 25.31 s\n[########################################] | 100% Completed | 25.32 s\n[########################################] | 100% Completed | 25.25 s\n[########################################] | 100% Completed | 127.40 s\n[########################################] | 100% Completed | 127.44 s\n[########################################] | 100% Completed | 127.49 s\n[########################################] | 100% Completed | 127.60 s\n[########################################] | 100% Completed | 127.62 s\n[########################################] | 100% Completed | 127.68 s\n[########################################] | 100% Completed | 127.72 s\n[########################################] | 100% Completed | 127.79 s\n[########################################] | 100% Completed | 127.82 s\n[########################################] | 100% Completed | 127.82 s\n[########################################] | 100% Completed | 127.92 s\n[########################################] | 100% Completed | 127.99 s\n[########################################] | 100% Completed | 128.01 s\n[########################################] | 100% Completed | 128.09 s\n[########################################] | 100% Completed | 101.45 ms\n[########################################] | 100% Completed | 201.38 ms\n[########################################] | 100% Completed | 203.00 ms\n[########################################] | 100% Completed | 302.74 ms\n[########################################] | 100% Completed | 403.37 ms\n[########################################] | 100% Completed | 504.06 ms\n[########################################] | 100% Completed | 602.26 ms\n[########################################] | 100% Completed | 603.28 ms\n[########################################] | 100% Completed | 703.13 ms\n[########################################] | 100% Completed | 803.56 ms\n[########################################] | 100% Completed | 903.36 ms\n[########################################] | 100% Completed | 1.00 s\n[########################################] | 100% Completed | 1.00 s\n[########################################] | 100% Completed | 1.10 s\n[########################################] | 100% Completed | 11.02 s\n[########################################] | 100% Completed | 11.02 s\n[########################################] | 100% Completed | 11.12 s\n[########################################] | 100% Completed | 11.22 s\n[########################################] | 100% Completed | 11.32 s\n[########################################] | 100% Completed | 11.42 s\n[########################################] | 100% Completed | 11.42 s\n[########################################] | 100% Completed | 11.52 s\n[########################################] | 100% Completed | 11.52 s\n[########################################] | 100% Completed | 11.62 s\n[########################################] | 100% Completed | 11.62 s\n[########################################] | 100% Completed | 11.72 s\n[########################################] | 100% Completed | 11.72 s\n[########################################] | 100% Completed | 11.82 s\n[########################################] | 100% Completed | 10.61 s\n[########################################] | 100% Completed | 10.71 s\n[########################################] | 100% Completed | 10.81 s\n[########################################] | 100% Completed | 10.81 s\n[########################################] | 100% Completed | 10.91 s\n[########################################] | 100% Completed | 10.91 s\n[########################################] | 100% Completed | 11.01 s\n[########################################] | 100% Completed | 11.11 s\n[########################################] | 100% Completed | 11.12 s\n[########################################] | 100% Completed | 11.22 s\n[########################################] | 100% Completed | 11.31 s\n[########################################] | 100% Completed | 11.41 s\n[########################################] | 100% Completed | 11.51 s\n[########################################] | 100% Completed | 11.61 s\n[########################################] | 100% Completed | 103.60 ms\n[########################################] | 100% Completed | 203.32 ms\n[########################################] | 100% Completed | 303.54 ms\n[########################################] | 100% Completed | 403.27 ms\n[########################################] | 100% Completed | 504.45 ms\n[########################################] | 100% Completed | 604.08 ms\n[########################################] | 100% Completed | 703.23 ms\n[########################################] | 100% Completed | 804.44 ms\n[########################################] | 100% Completed | 903.97 ms\n[########################################] | 100% Completed | 1.00 s\n[########################################] | 100% Completed | 1.01 s\n[########################################] | 100% Completed | 1.01 s\n[########################################] | 100% Completed | 1.10 s\n[########################################] | 100% Completed | 1.20 s\n[########################################] | 100% Completed | 12.50 s\n[########################################] | 100% Completed | 12.51 s\n[########################################] | 100% Completed | 12.59 s\n[########################################] | 100% Completed | 12.61 s\n[########################################] | 100% Completed | 12.69 s\n[########################################] | 100% Completed | 12.79 s\n[########################################] | 100% Completed | 12.79 s\n[########################################] | 100% Completed | 12.80 s\n[########################################] | 100% Completed | 12.81 s\n[########################################] | 100% Completed | 12.89 s\n[########################################] | 100% Completed | 12.89 s\n[########################################] | 100% Completed | 12.99 s\n[########################################] | 100% Completed | 13.09 s\n[########################################] | 100% Completed | 13.10 s\n[########################################] | 100% Completed | 207.24 ms\n[########################################] | 100% Completed | 302.79 ms\n[########################################] | 100% Completed | 304.91 ms\n[########################################] | 100% Completed | 403.58 ms\n[########################################] | 100% Completed | 406.88 ms\n[########################################] | 100% Completed | 506.57 ms\n[########################################] | 100% Completed | 605.63 ms\n[########################################] | 100% Completed | 705.91 ms\n[########################################] | 100% Completed | 804.67 ms\n[########################################] | 100% Completed | 905.48 ms\n[########################################] | 100% Completed | 909.89 ms\n[########################################] | 100% Completed | 1.01 s\n[########################################] | 100% Completed | 1.11 s\n[########################################] | 100% Completed | 1.11 s\nMemory usage: 351.64094829559326 MB\n[########################################] | 100% Completed | 101.93 ms\n[########################################] | 100% Completed | 102.83 ms\n[########################################] | 100% Completed | 201.95 ms\n[########################################] | 100% Completed | 204.61 ms\n[########################################] | 100% Completed | 301.95 ms\n[########################################] | 100% Completed | 303.59 ms\n[########################################] | 100% Completed | 403.08 ms\n[########################################] | 100% Completed | 502.54 ms\n[########################################] | 100% Completed | 503.27 ms\n[########################################] | 100% Completed | 603.17 ms\n[########################################] | 100% Completed | 604.45 ms\n[########################################] | 100% Completed | 703.00 ms\n[########################################] | 100% Completed | 708.58 ms\n[########################################] | 100% Completed | 806.08 ms\nNumber of rows: 24406269\n[########################################] | 100% Completed | 1.66 sms\n[########################################] | 100% Completed | 1.76 s\n[########################################] | 100% Completed | 1.85 s\n[########################################] | 100% Completed | 1.86 s\n[########################################] | 100% Completed | 1.93 s\n[########################################] | 100% Completed | 2.03 s\n[########################################] | 100% Completed | 2.06 s\n[########################################] | 100% Completed | 2.15 s\n[########################################] | 100% Completed | 2.16 s\n[########################################] | 100% Completed | 2.26 s\n[########################################] | 100% Completed | 2.36 s\n[########################################] | 100% Completed | 2.46 s\n[########################################] | 100% Completed | 2.55 s\n[########################################] | 100% Completed | 2.65 s\n[                                        ] | 0% Completed | 104.25 ms","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/3868329282.py:120: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n  .apply(pd.DataFrame.sort_values, ['day', 'day_minutes'])\\\n","output_type":"stream"},{"name":"stdout","text":"[########################################] | 100% Completed | 366.99 s\n[########################################] | 100% Completed | 367.09 s\n[########################################] | 100% Completed | 367.19 s\n[########################################] | 100% Completed | 367.29 s\n[########################################] | 100% Completed | 367.38 s\n[########################################] | 100% Completed | 367.39 s\n[########################################] | 100% Completed | 367.48 s\n[########################################] | 100% Completed | 367.49 s\n[########################################] | 100% Completed | 367.59 s\n[########################################] | 100% Completed | 367.68 s\n[########################################] | 100% Completed | 367.78 s\n[########################################] | 100% Completed | 367.88 s\n[########################################] | 100% Completed | 367.98 s\n[########################################] | 100% Completed | 367.98 s\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1394010/1394010 [04:05<00:00, 5672.38it/s] \n100%|██████████| 1394010/1394010 [00:06<00:00, 217987.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Malicious rate: 0.0\nFinal dataset size: 1394010\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the final parquet file\nfinal_df = dd.read_parquet(tmp_dir / 'final_4.2.parquet')\n\n# Define the output CSV path with a filename\noutput_csv_path = Path('/kaggle/working/final_data.csv')\n\n# Save the DataFrame as a CSV\nfinal_df.to_csv(output_csv_path, index=False)\n\nprint(f\"Final dataset saved as CSV at {output_csv_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:34:18.147142Z","iopub.execute_input":"2024-10-21T09:34:18.147997Z","iopub.status.idle":"2024-10-21T09:39:18.543321Z","shell.execute_reply.started":"2024-10-21T09:34:18.147963Z","shell.execute_reply":"2024-10-21T09:39:18.542383Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"[########################################] | 100% Completed | 299.27 s\n[########################################] | 100% Completed | 299.37 s\n[########################################] | 100% Completed | 299.48 s\n[########################################] | 100% Completed | 299.57 s\n[########################################] | 100% Completed | 299.67 s\n[########################################] | 100% Completed | 299.67 s\n[########################################] | 100% Completed | 299.77 s\n[########################################] | 100% Completed | 299.87 s\n[########################################] | 100% Completed | 299.97 s\n[########################################] | 100% Completed | 299.97 s\n[########################################] | 100% Completed | 300.07 s\n[########################################] | 100% Completed | 300.17 s\n[########################################] | 100% Completed | 300.27 s\n[########################################] | 100% Completed | 300.36 s\nFinal dataset saved as CSV at /kaggle/working/final_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the parquet files\nfinal_df = pd.read_parquet('/kaggle/working/tmp/final_4.2.parquet')\naggregated_df = pd.read_parquet('/kaggle/working/tmp/df_aggregated.parquet')\n\n# Define the output directory where CSVs will be saved\noutput_csv_dir = '/kaggle/working/tmp/'\n\n# Convert and save the final_4.2.parquet file to CSV\nfinal_csv_path = output_csv_dir + 'final_4.2.csv'\nfinal_df.to_csv(final_csv_path, index=False)\nprint(f\"final_4.2.parquet converted to CSV and saved at {final_csv_path}\")\n\n# Convert and save the df_aggregated.parquet file to CSV\naggregated_csv_path = output_csv_dir + 'df_aggregated.csv'\naggregated_df.to_csv(aggregated_csv_path, index=False)\nprint(f\"df_aggregated.parquet converted to CSV and saved at {aggregated_csv_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T09:50:23.480722Z","iopub.execute_input":"2024-10-21T09:50:23.481592Z","iopub.status.idle":"2024-10-21T09:59:10.188165Z","shell.execute_reply.started":"2024-10-21T09:50:23.481555Z","shell.execute_reply":"2024-10-21T09:59:10.186949Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"final_4.2.parquet converted to CSV and saved at /kaggle/working/tmp/final_4.2.csv\ndf_aggregated.parquet converted to CSV and saved at /kaggle/working/tmp/df_aggregated.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**BERT**","metadata":{}},{"cell_type":"code","source":"# Clone the repository to your Kaggle environment\n!git clone https://github.com/xelav/Insider-Threat-CERT.git /kaggle/working/repo\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:02:51.935973Z","iopub.execute_input":"2024-10-21T10:02:51.936354Z","iopub.status.idle":"2024-10-21T10:02:53.985678Z","shell.execute_reply.started":"2024-10-21T10:02:51.936322Z","shell.execute_reply":"2024-10-21T10:02:53.984150Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/repo'...\nremote: Enumerating objects: 335, done.\u001b[K\nremote: Counting objects: 100% (25/25), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 335 (delta 20), reused 19 (delta 19), pack-reused 310 (from 1)\u001b[K\nReceiving objects: 100% (335/335), 1.51 MiB | 20.35 MiB/s, done.\nResolving deltas: 100% (222/222), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nrepo_dir = \"/kaggle/working/repo\"\nos.chdir(repo_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:03:02.866312Z","iopub.execute_input":"2024-10-21T10:03:02.866700Z","iopub.status.idle":"2024-10-21T10:03:02.871799Z","shell.execute_reply.started":"2024-10-21T10:03:02.866665Z","shell.execute_reply":"2024-10-21T10:03:02.870745Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(repo_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:03:13.599990Z","iopub.execute_input":"2024-10-21T10:03:13.600380Z","iopub.status.idle":"2024-10-21T10:03:13.604830Z","shell.execute_reply.started":"2024-10-21T10:03:13.600345Z","shell.execute_reply":"2024-10-21T10:03:13.603845Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-ignite\n!pip install ipdb\n!pip install transformers\n!pip install tokenizers\n%pip install wandb==0.8.18 -q","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:03:23.081475Z","iopub.execute_input":"2024-10-21T10:03:23.081828Z","iopub.status.idle":"2024-10-21T10:04:36.519055Z","shell.execute_reply.started":"2024-10-21T10:03:23.081801Z","shell.execute_reply":"2024-10-21T10:04:36.517849Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-ignite in /opt/conda/lib/python3.10/site-packages (0.4.12)\nRequirement already satisfied: torch<3,>=1.3 in /opt/conda/lib/python3.10/site-packages (from pytorch-ignite) (2.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytorch-ignite) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytorch-ignite) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\nCollecting ipdb\n  Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\nRequirement already satisfied: ipython>=7.31.1 in /opt/conda/lib/python3.10/site-packages (from ipdb) (8.14.0)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from ipdb) (2.0.1)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipdb) (5.1.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.2.0)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.18.2)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.1.6)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (3.0.38)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (2.15.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (5.9.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.31.1->ipdb) (0.2.6)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (0.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.16.0)\nInstalling collected packages: ipdb\nSuccessfully installed ipdb-0.13.13\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.13.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade wandb\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:09:23.196236Z","iopub.execute_input":"2024-10-21T10:09:23.196889Z","iopub.status.idle":"2024-10-21T10:09:38.313667Z","shell.execute_reply.started":"2024-10-21T10:09:23.196855Z","shell.execute_reply":"2024-10-21T10:09:38.312456Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.8.18)\nCollecting wandb\n  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.10.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nCollecting sentry-sdk>=2.0.0 (from wandb)\n  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.5/314.5 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.6.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nInstalling collected packages: sentry-sdk, wandb\n  Attempting uninstall: sentry-sdk\n    Found existing installation: sentry-sdk 1.30.0\n    Uninstalling sentry-sdk-1.30.0:\n      Successfully uninstalled sentry-sdk-1.30.0\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.8.18\n    Uninstalling wandb-0.8.18:\n      Successfully uninstalled wandb-0.8.18\nSuccessfully installed sentry-sdk-2.17.0 wandb-0.18.5\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom pathlib import Path\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport shutil\nfrom tqdm import tqdm\nimport gc\n\nimport torch.utils.data\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator, Engine\nfrom ignite.metrics import Accuracy, Loss\n\nfrom src.models import InsiderClassifier, LSTM_Encoder, LSTM_Encoder_Topics, LSTM_Encoder_Numeric\nfrom src.params import get_params\nfrom src.dataset import CertDataset, create_data_loaders\nfrom src.lstm_trainer import *\n\nimport ipdb\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:48:04.727788Z","iopub.execute_input":"2024-10-21T10:48:04.728536Z","iopub.status.idle":"2024-10-21T10:48:05.296664Z","shell.execute_reply.started":"2024-10-21T10:48:04.728494Z","shell.execute_reply":"2024-10-21T10:48:05.295343Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[144], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_params\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CertDataset, create_data_loaders\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlstm_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m\n","File \u001b[0;32m/kaggle/working/repo/src/lstm_trainer.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard_logger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint, DiskSaver\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AccuracyIgnoringPadding\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_batch_lstm\u001b[39m(\n\u001b[1;32m     15\u001b[0m         batch,\n\u001b[1;32m     16\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m         non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     19\u001b[0m         train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     25\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/__init__.py:25\u001b[0m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_types, env, util\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WBValue\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/apis/__init__.py:37\u001b[0m\n\u001b[1;32m     32\u001b[0m         requests\u001b[38;5;241m.\u001b[39mSession\u001b[38;5;241m.\u001b[39mmerge_environment_settings \u001b[38;5;241m=\u001b[39m old_merge_environment_settings\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reset\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_disabled\u001b[49m():\n\u001b[1;32m     38\u001b[0m     _disable_ssl()\n\u001b[1;32m     41\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n","\u001b[0;31mAttributeError\u001b[0m: module 'wandb.env' has no attribute 'ssl_disabled'"],"ename":"AttributeError","evalue":"module 'wandb.env' has no attribute 'ssl_disabled'","output_type":"error"}]},{"cell_type":"code","source":"# output_dir = Path(r'C:\\Users\\Mvideo\\Google Drive\\Datasets\\CERT_output')\n# answers_dir = Path(r\"C:/Users/Mvideo/Downloads/answers\")\n\ndataset_version = '4.2'\n\noutput_dir = Path(f'/kaggle/input/cert-insider-threat-detection-research')\nanswers_dir = Path(f\"/kaggle/input/answers\")\nmain_answers_file = answers_dir / \"insiders.csv\"\n\nassert(output_dir.is_dir())\nassert(answers_dir.is_dir())\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-21T10:29:36.482829Z","iopub.execute_input":"2024-10-21T10:29:36.483518Z","iopub.status.idle":"2024-10-21T10:29:36.558199Z","shell.execute_reply.started":"2024-10-21T10:29:36.483483Z","shell.execute_reply":"2024-10-21T10:29:36.557190Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}