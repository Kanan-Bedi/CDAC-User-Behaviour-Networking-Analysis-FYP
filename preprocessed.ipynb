{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":6732710,"sourceType":"datasetVersion","datasetId":3833605},{"sourceId":9643561,"sourceType":"datasetVersion","datasetId":5889155}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T17:35:21.975950Z","iopub.execute_input":"2024-10-16T17:35:21.976206Z","iopub.status.idle":"2024-10-16T17:35:23.780648Z","shell.execute_reply.started":"2024-10-16T17:35:21.976181Z","shell.execute_reply":"2024-10-16T17:35:23.779788Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install dask","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:40:30.680211Z","iopub.execute_input":"2024-10-16T17:40:30.680549Z","iopub.status.idle":"2024-10-16T17:40:36.433928Z","shell.execute_reply.started":"2024-10-16T17:40:30.680521Z","shell.execute_reply":"2024-10-16T17:40:36.432758Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting dask\n  Downloading dask-2024.9.1-py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/site-packages (from dask) (3.0.0)\nRequirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/site-packages (from dask) (8.1.7)\nCollecting partd>=1.4.0\n  Downloading partd-1.4.2-py3-none-any.whl (18 kB)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from dask) (24.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/site-packages (from dask) (6.0.2)\nRequirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/site-packages (from dask) (2024.9.0)\nRequirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/site-packages (from dask) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/site-packages (from dask) (8.5.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask) (3.20.2)\nCollecting locket\n  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\nInstalling collected packages: locket, partd, dask\nSuccessfully installed dask-2024.9.1 locket-1.0.0 partd-1.4.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport dask.dataframe as dd","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:44:08.925594Z","iopub.execute_input":"2024-10-16T17:44:08.927057Z","iopub.status.idle":"2024-10-16T17:44:08.931217Z","shell.execute_reply.started":"2024-10-16T17:44:08.926985Z","shell.execute_reply":"2024-10-16T17:44:08.930389Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install gensim","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:45:10.593130Z","iopub.execute_input":"2024-10-16T17:45:10.593493Z","iopub.status.idle":"2024-10-16T17:45:23.725436Z","shell.execute_reply.started":"2024-10-16T17:45:10.593461Z","shell.execute_reply":"2024-10-16T17:45:23.724423Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting gensim\n  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting smart-open>=1.8.1\n  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/site-packages (from gensim) (1.26.4)\nCollecting scipy<1.14.0,>=1.7.0\n  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\nInstalling collected packages: smart-open, scipy, gensim\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\nSuccessfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.0.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nfrom pathlib import Path\nimport re\nfrom gensim.models import TfidfModel, nmf\nfrom gensim.corpora import Dictionary as Dict\nfrom gensim.models.ldamulticore import LdaModel\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom gensim.models.nmf_pgd import solve_h","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:46:02.750497Z","iopub.execute_input":"2024-10-16T17:46:02.751431Z","iopub.status.idle":"2024-10-16T17:46:03.050606Z","shell.execute_reply.started":"2024-10-16T17:46:02.751386Z","shell.execute_reply":"2024-10-16T17:46:03.049905Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"CHUNK_SIZE = 10000\nTOPIC_NUM = 100\nALLFILES = ['logon.csv','device.csv', 'email.csv', 'file.csv', 'http.csv']\nCONTENT_FILES = ['email.csv', 'file.csv', 'http.csv']","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:46:13.904689Z","iopub.execute_input":"2024-10-16T17:46:13.905424Z","iopub.status.idle":"2024-10-16T17:46:13.909524Z","shell.execute_reply.started":"2024-10-16T17:46:13.905388Z","shell.execute_reply":"2024-10-16T17:46:13.908879Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def check():\n    if not output_dir.is_dir():\n        os.makedirs(output_dir)\n    assert (answers_dir.is_dir())\n    assert (dataset_dir.is_dir())\n    assert (main_answers_file.is_file())\n    assert (output_dir.is_dir())","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:05.173422Z","iopub.execute_input":"2024-10-16T17:47:05.173825Z","iopub.status.idle":"2024-10-16T17:47:05.178712Z","shell.execute_reply.started":"2024-10-16T17:47:05.173795Z","shell.execute_reply":"2024-10-16T17:47:05.178064Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def count_file_lines(file):\n    with open(file) as f:\n        for count, _ in enumerate(f):\n            pass\n    return count","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:12.050180Z","iopub.execute_input":"2024-10-16T17:47:12.050577Z","iopub.status.idle":"2024-10-16T17:47:12.055118Z","shell.execute_reply.started":"2024-10-16T17:47:12.050547Z","shell.execute_reply":"2024-10-16T17:47:12.054452Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def collect_vocabulary(csv_name):\n    result_set = set()\n\n    for df in pd.read_csv(dataset_dir / f'{csv_name}.csv',\n                          usecols=['date', 'user', 'content'], chunksize=CHUNK_SIZE):\n        df['content'] = df['content'].str.lower().str.split()\n        result_set = result_set.union(*map(set, df['content']))\n\n    return result_set","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:19.704509Z","iopub.execute_input":"2024-10-16T17:47:19.705455Z","iopub.status.idle":"2024-10-16T17:47:19.709888Z","shell.execute_reply.started":"2024-10-16T17:47:19.705419Z","shell.execute_reply":"2024-10-16T17:47:19.709220Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def chunk_iterator(filename):\n    for chunk in pd.read_csv(filename, chunksize=CHUNK_SIZE):\n        for document in chunk['content'].str.lower().str.split().values:\n            yield document\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:27.453513Z","iopub.execute_input":"2024-10-16T17:47:27.454377Z","iopub.status.idle":"2024-10-16T17:47:27.458457Z","shell.execute_reply.started":"2024-10-16T17:47:27.454337Z","shell.execute_reply":"2024-10-16T17:47:27.457804Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def tfidf_iterator(filenames, dictionary):\n    for filename in filenames:\n        for chunk in pd.read_csv(dataset_dir /filename, chunksize=CHUNK_SIZE):\n            for document in chunk['content'].str.lower().str.split().values:\n                yield dictionary.doc2bow(document)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:32.667544Z","iopub.execute_input":"2024-10-16T17:47:32.668430Z","iopub.status.idle":"2024-10-16T17:47:32.672565Z","shell.execute_reply.started":"2024-10-16T17:47:32.668394Z","shell.execute_reply":"2024-10-16T17:47:32.671881Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def nmf_iterator(filenames, dictionary, tfidf):\n    for filename in filenames:\n        for chunk in pd.read_csv(dataset_dir /filename, chunksize=CHUNK_SIZE):\n            for document in chunk['content'].str.lower().str.split().values:\n                yield tfidf[dictionary.doc2bow(document)]","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:38.660210Z","iopub.execute_input":"2024-10-16T17:47:38.660602Z","iopub.status.idle":"2024-10-16T17:47:38.665581Z","shell.execute_reply.started":"2024-10-16T17:47:38.660571Z","shell.execute_reply":"2024-10-16T17:47:38.664784Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def parallelize(data, func, num_of_processes=8):\n    data_split = np.array_split(data, num_of_processes)\n    pool = Pool(num_of_processes)\n    data = pd.concat(pool.map(func, data_split))\n    pool.close()\n    pool.join()\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:44.165064Z","iopub.execute_input":"2024-10-16T17:47:44.165493Z","iopub.status.idle":"2024-10-16T17:47:44.170053Z","shell.execute_reply.started":"2024-10-16T17:47:44.165459Z","shell.execute_reply":"2024-10-16T17:47:44.169397Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def run_on_subset(func, data_subset):\n    return data_subset.apply(func, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:50.529152Z","iopub.execute_input":"2024-10-16T17:47:50.529923Z","iopub.status.idle":"2024-10-16T17:47:50.533507Z","shell.execute_reply.started":"2024-10-16T17:47:50.529889Z","shell.execute_reply":"2024-10-16T17:47:50.532858Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def parallelize_on_rows(data, func, num_of_processes=8):\n    return parallelize(data, partial(run_on_subset, func), num_of_processes)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:47:56.965455Z","iopub.execute_input":"2024-10-16T17:47:56.966437Z","iopub.status.idle":"2024-10-16T17:47:56.970090Z","shell.execute_reply.started":"2024-10-16T17:47:56.966397Z","shell.execute_reply":"2024-10-16T17:47:56.969453Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def process_content(filename, chunk_size=CHUNK_SIZE):\n    model = LdaModel.load((output_dir / 'lda_model.pkl').as_posix())\n    temp_dict = Dict.load((output_dir / 'dict.pkl').as_posix())\n    out_file = output_dir / (filename.stem + '_lda.csv')\n    # touch one while not exist\n    if not out_file.is_file():\n        Path(out_file).touch()\n\n    for chunk in pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size):\n        chunk['content'] = chunk['content'].str.lower().str.split() \\\n            .apply(lambda doc: model[temp_dict.doc2bow(doc)])\n\n        chunk.to_csv(out_file, mode='a')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:48:16.711614Z","iopub.execute_input":"2024-10-16T17:48:16.712382Z","iopub.status.idle":"2024-10-16T17:48:16.717806Z","shell.execute_reply.started":"2024-10-16T17:48:16.712345Z","shell.execute_reply":"2024-10-16T17:48:16.717065Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def pre_process_logon(path):\n    df = pd.read_csv(path)\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S').dt.floor('D')\n    df['day']=df['date'].dt.dayofweek\n    \n    self_pc = df \\\n        .groupby(['user', 'day', 'pc']).size().to_frame('count') \\\n        .reset_index().sort_values('count', ascending=False) \\\n        .drop_duplicates(subset=['user', 'day']) \\\n        .drop(columns=['count']).sort_values(['user', 'day']) \\\n        .groupby('user').pc.agg(pd.Series.mode).rename('self_pc')\n    df = df.merge(self_pc.to_frame(), left_on='user', right_on='user')\n    print(\"Done\")\n    print(df.head()) \n    #df['is_usual_pc'] = df['self_pc'] == df['pc']\n   \n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n    \n    df['subtype'] = df['activity']\n    #df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / 'logon_preprocessed.csv')\n    df[['id', 'date', 'user', 'is_work_time', 'subtype']].to_csv(output_dir / 'logon_preprocessed.csv')\n    return self_pc.to_frame()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:48:25.328991Z","iopub.execute_input":"2024-10-16T17:48:25.329443Z","iopub.status.idle":"2024-10-16T17:48:25.336649Z","shell.execute_reply.started":"2024-10-16T17:48:25.329408Z","shell.execute_reply":"2024-10-16T17:48:25.335995Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def pre_process_device(path):\n    df = pd.read_csv(path)\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n    df = df.merge(self_pc, left_on='user', right_on='user', )\n    #df['is_usual_pc'] = df['self_pc'] == df['pc']\n\n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n\n    df['subtype'] = df['activity']\n    #df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n    #    output_dir / f'device_preprocessed.csv')\n    df[['id', 'date', 'user', 'is_work_time', 'subtype']].to_csv(\n        output_dir / f'device_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:48:48.222412Z","iopub.execute_input":"2024-10-16T17:48:48.223184Z","iopub.status.idle":"2024-10-16T17:48:48.228130Z","shell.execute_reply.started":"2024-10-16T17:48:48.223147Z","shell.execute_reply":"2024-10-16T17:48:48.227467Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def pre_process_file(path):\n    df = pd.read_csv(path, usecols=['id', 'date', 'user', 'pc', 'filename'])\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n\n    df = df.merge(self_pc, left_on='user', right_on='user', )\n    #df['is_usual_pc'] = df['self_pc'] == df['pc']\n\n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n\n    file_extensions = df.filename.str[-4:]\n    df['subtype'] = file_extensions\n    #df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n    #    output_dir / f'file_preprocessed.csv')\n    df[['id', 'date', 'user', 'is_work_time', 'subtype']].to_csv(\n        output_dir / f'file_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:48:54.749297Z","iopub.execute_input":"2024-10-16T17:48:54.750297Z","iopub.status.idle":"2024-10-16T17:48:54.756799Z","shell.execute_reply.started":"2024-10-16T17:48:54.750255Z","shell.execute_reply":"2024-10-16T17:48:54.756083Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def pre_process_email(path):\n    df = pd.read_csv(path, usecols=['id', 'date', 'user', 'pc', 'to', 'cc', 'bcc', 'from'])\n    df = df.fillna('')\n    to_concated = df[['to', 'cc', 'bcc']].apply(lambda x: ';'.join([x.to, x.cc, x.bcc]), axis=1)\n    is_external_to = to_concated.apply(\n        lambda x: any([re.match('^.+@(.+$)', e).group(1) != 'dtaa.com' for e in x.split(';') if e != '']))\n    is_external = is_external_to | is_external_to\n    df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n\n    df = df.merge(self_pc, left_on='user', right_on='user', )\n    df['is_usual_pc'] = df['self_pc'] == df['pc']\n\n    is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n    df['is_work_time'] = is_work_time\n\n    df['subtype'] = is_external\n    df[['id', 'date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(\n        output_dir / f'email_preprocessed.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:49:02.051020Z","iopub.execute_input":"2024-10-16T17:49:02.052069Z","iopub.status.idle":"2024-10-16T17:49:02.058966Z","shell.execute_reply.started":"2024-10-16T17:49:02.051995Z","shell.execute_reply":"2024-10-16T17:49:02.058305Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def pre_process_http(path):\n\n    # scenario 1\n    scenario_1_http = [\n        'actualkeylogger.com',\n        'best-spy-soft.com',\n        'dailykeylogger.com',\n        'keylogpc.com',\n        'refog.com',\n        'relytec.com',\n        'softactivity.com',\n        'spectorsoft.com',\n        'webwatchernow.com',\n        'wellresearchedreviews.com',\n        'wikileaks.org'\n    ]\n\n    # scenario 2\n    scenario_2_http = [\n        'careerbuilder.com',\n        'craiglist.org',\n        'indeed.com',\n        'job-hunt.org',\n        'jobhuntersbible.com',\n        'linkedin.com',\n        'monster.com',\n        'simplyhired.com',\n    ]\n\n    \n     #scenario 3\n    scenario_3_http = [\n        '4shared.com'\n        'dropbox.com',\n        'fileserve.com',\n        'filefreak.com',\n        'filestube.com',\n        'megaupload.com',\n        'thepiratebay.org'\n    ]\n\n    first_it = True\n    mode = 'w'\n\n    for http_df in pd.read_csv(path, chunksize=CHUNK_SIZE, usecols=['id','date', 'user', 'pc', 'url']):\n        http_df['date'] = pd.to_datetime(http_df.date, format='%m/%d/%Y %H:%M:%S')\n\n        site_names = http_df['url'].apply(lambda s: re.match('^https?://(www)?([0-9\\-\\w\\.]+)?.+$', s).group(2))\n        http_df['site_name'] = site_names\n\n        http_df['subtype'] = 0\n        http_df.loc[site_names.isin(scenario_1_http), 'subtype'] = 1\n        http_df.loc[site_names.isin(scenario_2_http), 'subtype'] = 2\n        http_df.loc[site_names.isin(scenario_3_http), 'subtype'] = 3\n\n        http_df = http_df.merge(self_pc, left_on='user', right_on='user', )\n        #http_df['is_usual_pc'] = http_df['self_pc'] == http_df['pc']\n\n        is_work_time = (8 <= http_df.date.dt.hour) & (http_df.date.dt.hour < 17)\n        http_df['is_work_time'] = is_work_time\n\n        http_df.to_csv(output_dir / 'http_preprocessed.csv', header=first_it, index=False,\n                       mode=mode, columns=['id', 'date', 'user', 'is_work_time', 'subtype', 'site_name'])\n        first_it = False\n        mode = 'a'","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:50:04.973192Z","iopub.execute_input":"2024-10-16T17:50:04.973645Z","iopub.status.idle":"2024-10-16T17:50:04.982457Z","shell.execute_reply.started":"2024-10-16T17:50:04.973614Z","shell.execute_reply":"2024-10-16T17:50:04.981779Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def merge_all_content():\n    df_dict = Dict(chunk_iterator(dataset_dir / 'email.csv'))\n    df_dict.add_documents(chunk_iterator(dataset_dir / 'file.csv'))\n    df_dict.add_documents(chunk_iterator(dataset_dir / 'http.csv'))\n\n    df_dict.save((output_dir / 'dict.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:50:18.704786Z","iopub.execute_input":"2024-10-16T17:50:18.705551Z","iopub.status.idle":"2024-10-16T17:50:18.709904Z","shell.execute_reply.started":"2024-10-16T17:50:18.705515Z","shell.execute_reply":"2024-10-16T17:50:18.709183Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def make_tfidf_model():\n    tfidf_model = TfidfModel(\n        tfidf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix())))\n\n    tfidf_model.save((output_dir / 'tfidf_model.pkl').as_posix())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:50:23.332377Z","iopub.execute_input":"2024-10-16T17:50:23.333235Z","iopub.status.idle":"2024-10-16T17:50:23.337281Z","shell.execute_reply.started":"2024-10-16T17:50:23.333194Z","shell.execute_reply":"2024-10-16T17:50:23.336597Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def make_nmf_model():\n    tfidf_model = TfidfModel.load((output_dir / 'tfidf_model.pkl').as_posix())\n    nmf_model = nmf.Nmf(\n        nmf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix()),\n                     tfidf_model), num_topics=TOPIC_NUM)\n    nmf_model.save((output_dir / 'nmf_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:50:31.660314Z","iopub.execute_input":"2024-10-16T17:50:31.660737Z","iopub.status.idle":"2024-10-16T17:50:31.665421Z","shell.execute_reply.started":"2024-10-16T17:50:31.660707Z","shell.execute_reply":"2024-10-16T17:50:31.664755Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def make_lda_model():\n    tfidf_model = TfidfModel.load((output_dir / 'tfidf_model.pkl').as_posix())\n    lda_model = LdaModel(\n        nmf_iterator(CONTENT_FILES, Dict.load((output_dir / 'dict.pkl').as_posix()),\n                     tfidf_model), num_topics=TOPIC_NUM)\n    lda_model.save((output_dir / 'lda_model.pkl').as_posix())","metadata":{"execution":{"iopub.status.busy":"2024-10-16T17:50:37.450807Z","iopub.execute_input":"2024-10-16T17:50:37.451754Z","iopub.status.idle":"2024-10-16T17:50:37.455851Z","shell.execute_reply.started":"2024-10-16T17:50:37.451718Z","shell.execute_reply":"2024-10-16T17:50:37.455189Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    #if len(sys.argv) != 3:\n    #    print(\"Usage:\", sys.argv[0], \"[answer directory] [dataset directory]\")\n    #    print(\"Warning: may cost over 30 hours to run all steps for the dataset over 20G\")\n    #    print(\"hint: comment out the pre_runned process\")\n\n    #print(\"Start to process dataset ver\", sys.argv[2].split('/')[-1])\n    answers_dir = Path(\"./answers\")\n    dataset_dir = Path(\"./dataset\")\n    main_answers_file = answers_dir / \"insiders.csv\"\n    output_dir = Path('./_output/')\n\n    if not os.path.isdir(output_dir):  # case of no _output directory\n        os.mkdir(output_dir)\n\n    self_pc = pre_process_logon(dataset_dir / '/kaggle/input/cert-insider-threat-detection-research/logon.csv')\n    print(\"logon processed\")\n\n    pre_process_device(dataset_dir / '/kaggle/input/cert-insider-threat-detection-research/device.csv')\n    print(\"device processed\")\n\n    pre_process_file(dataset_dir / '/kaggle/input/cert-insider-threat-detection-research/file.csv')\n    print(\"file processed\")\n\n    #pre_process_email(dataset_dir / '/kaggle/input/cert-insider-threat-detection-research/email.csv')\n    #print(\"email processed\")\n\n    pre_process_http(dataset_dir / '/kaggle/input/cert-insider-threat-detection-research/http.csv')\n    print(\"http processed\")\n\n    merge_all_content()\n    print(\"all content file merged and saved\")\n    make_tfidf_model()\n    print(\"tfidf model saved\")\n    make_nmf_model()\n    print(\"nmf model saved\")\n    make_lda_model()\n    print(\"lda model saved\")\n\n    #pre process all content files\n    for file in CONTENT_FILES:\n        process_content(dataset_dir / file)\n        print(file, \"content processed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}